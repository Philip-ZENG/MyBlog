---
layout: post
title: "AI Insight Report"
date: 2025-06-17
---

![]({{ '/image/cover_page_1.png' | relative_url }})

# 1. AI科技

## 1.1 AI技术发展回顾

### 1.1.1 大语言模型技术里程碑及其影响

#### 1.1.1.1 大语言模型技术里程碑事件

- 大语言模型发展的重要时间节点：包括论文提出、模型发布、计算芯片发布

| 类别     | 时间    | 事件                                                         | 发布组织                | 介绍&意义                                                    |
| -------- | ------- | ------------------------------------------------------------ | ----------------------- | ------------------------------------------------------------ |
| 学术文章 | 2014.9  | "Neural Machine Translation by Jointly Learning to Align and Translate" | Dzmitry Bahdanau et al. | 首次提出注意力机制，并应用于循环神经网络                     |
| GPU发布  | 2017.5  | NVIDIA Tesla V100                                            | NVIDIA                  | 它基于 Volta 架构，引入了 Tensor Core，显著加速了深度学习计算。成为 LLM 早期至中期训练的主流设备 |
| 学术文章 | 2017.7  | "Attention is All You Need"                                  | Google                  | 完全放弃循环/卷积架构，首次提出纯粹基于自注意力机制的机器学习模型，并达到SOTA表现 |
| GPU发布  | 2018.5  | Google TPU v3                                                | Google                  | 谷歌的第三代人工智能加速器，提供显著的性能提升和液体冷却，为 BERT 等许多内部模型提供动力 |
| 学术文章 | 2018.6  | GPT-1                                                        | OpenAI                  | Paper "Improving Language Understanding by Generative Pre-Training"，首次提出Generative Pre-trained Transformer (GPT)这一decoder-only模型，提出了"无监督预训练+监督微调"训练范式，117 million参数量，使用NVIDIA V100训练 |
| 学术文章 | 2018.10 | BERT                                                         | Google                  | Paper "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"，首次提出BERT这一encoder-only模型，提出masked language modeling (MLM)，一种无监督学习方法，常用于预训练，110 million参数量，使用Google TPU v2 / v3训练 |
| 学术文章 | 2019.2  | GPT-2                                                        | OpenAI                  | Paper "Language Models are Unsupervised Multitask Learners"，达到1.5 billion参数量，使用NVIDIA V100训练 |
| 学术文章 | 2020.1  | "Scaling Laws for Neural Language Models"                    | OpenAI                  | 提供了经验证据，论证 LLM 性能随着模型大小、数据集大小和计算量的增加而可预测地扩展这一Scaling Law，为业内后续构建更大模型提供动力 |
| GPU发布  | 2020.5  | NVIDIA A100                                                  | NVIDIA                  | 它基于安培架构，带来了重大的性能飞跃、更大的HBM2e内存（高达80GB）以及NVLink 3.0，成为训练GPT-3、Llama 1/2等海量模型的主导GPU |
| 学术文章 | 2020.5  | GPT-3                                                        | OpenAI                  | Paper "Language Models are Few-Shot Learners"，发布GPT-3，达到175 billion参数量，实现参数量的飞跃，展现出极强的"few-shot" and "zero-shot" 学习能力，发布API供世界开发者使用，使用 Microsoft Azure上部署的~10,000 V100 GPUs训练 |
| 公司成立 | 2021.1  | Anthropic Founded                                            | Anthropic               | Anthropic由前OpenAI研究员成立，关注于研发安全的AI模型        |
| GPU发布  | 2021.5  | Google TPU v4                                                | Google                  | 谷歌的第四代产品，提供 v3 两倍的性能和改进的互连，对于训练 PaLM 和 LaMDA 至关重要 |
| 学术文章 | 2021.8  | "On the Opportunities and Risks of Foundation Models"        | Stanford University     | 正式提出了Foundation model的定义                             |
| 学术文章 | 2022.1  | Google paper "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" | Google                  | 提出了Chain-of-Thought (CoT) prompting，这一技巧通过鼓励模型生成中间推理步骤，显著提高了 LLM 在推理任务上的表现。 |
| 学术文章 | 2022.3  | OpenAI InstructGPT Paper "Training language models to follow instructions with human feedback" | OpenAI                  | 介绍Reinforcement Learning from Human Feedback (RLHF)，这一方法使 LLM 与用户意图保持一致，使其更加有用且无害 |
| GPU发布  | 2022.3  | NVIDIA H100                                                  | NVIDIA                  | 它基于 Hopper 架构，引入了 Transformer Engine、80GB HBM3 内存和 NVLink 4.0。自 2022 年末起，它成为训练 SOTA 模型（Llama 3、GPT-4、Claude 3 等）最抢手的 GPU |
| 学术文章 | 2022.4  | Google PaLM                                                  | Google                  | Paper "PaLM: Scaling Language Modeling with Pathways"，达到540 billion参数量，是Google用于与OpenAI GPT系列模型抗衡的模型，使用Google TPU v4训练 |
| 模型发布 | 2022.11 | ChatGPT                                                      | OpenAI                  | 这一事件标志着一个转折点，使大语言模型进入了主流公众视野，ChatGPT使用GPT-3.5微调后的模型，其直观的对话界面和令人惊讶的强大能力导致了病毒式传播，引发了关于人工智能的激烈讨论和行业军备竞赛，使用Microsoft Azure上部署的NVIDIA A100训练 |
| 模型发布 | 2023.2  | LLaMA                                                        | Meta                    | Paper "LLaMA: Open and Efficient Foundation Language Models"，7B到65B参数量，这一开源模型的发布引发了对开源模型进行微调和开发的热潮，使用NVIDIA A100训练 |
| 模型发布 | 2023.3  | GPT-4                                                        | OpenAI                  | Paper "GPT-4 Technical Report"，展现了多模态功能（接受文本和图像输入），在Microsoft Azure上训练，预计使用了~25,000个NVIDIA A100 / H100训练，估计使用了MoE架构，1.8T参数 |
| 模型发布 | 2023.3  | Claude                                                       | Anthropic               | Anthropic发布Claude模型，作为OpenAI ChatGPT的竞争对手，使用AWS NVIDIA集群和Google TPUs训练 |
| 模型发布 | 2023.7  | Llama 2                                                      | Meta                    | Paper "Llama 2: Open Foundation and Fine-Tuned Chat Models"，Meta发布Llama 2，进一步促进了开源LLM的开发，使用NVIDIA A100训练 |
| GPU发布  | 2023.8  | Google TPU v5e                                               | NVIDIA                  | 一款注重效率的 TPU，专为推理和经济高效的训练/服务而设计。由 Anthropic 使用 |
| GPU发布  | 2023.12 | Google TPU v5p                                               | NVIDIA                  | 谷歌迄今为止最强大的 TPU，专为大规模训练集群（最多 8,960 个芯片）而设计，为 Gemini 提供动力 |
| 模型发布 | 2023.12 | Gemini                                                       | Google                  | Google原生多模态模型，与GPT-4直接竞争，使用Google TPU v5p训练 |
| 模型发布 | 2024.2  | Gemini 1.5 Pro                                               | Google                  | 1 million token上下文窗口，使用了Mixture-of-Experts (MoE) 架构 |
| 模型发布 | 2024.2  | OpenAI Sora                                                  | OpenAI                  | 文字生成视频模型，根据文本提示生成高保真、相对较长的视频片段的能力展示了生成人工智能在文本和图像之外的重大飞跃 |
| 模型发布 | 2024.2  | Google Gemma                                                 | Google                  | 轻量级、开源模型（2B 和 7B 参数）                            |
| 模型发布 | 2024.3  | Claude 3 Family                                              | Anthropic               | Haiku, Sonnet, and Opus模型，Opus 声称在多个行业基准上均优于 GPT-4，包括本科水平的知识和基础数学，并且具有强大的多模态（图像和文本）功能 |
| GPU发布  | 2024.3  | NVIDIA B100                                                  | NVIDIA                  | 基于 Blackwell 架构，承诺性能将大幅提升，尤其是在推理和训练万亿参数模型方面。预计于 2024 年末/2025 年初上市，并将为下一代 LLM 提供支持 |
| 模型发布 | 2024.4  | Llama 3                                                      | Meta                    | Open-weight开源模型，8B and 70B参数，使用NVIDIA H100训练     |
| 模型发布 | 2024.5  | GPT-4o ("omni")                                              | OpenAI                  | 跨文本、音频和视觉的原生多模态，提供更快的响应时间并提供 GPT-4 级智能，使用Microsoft Azure H100集群训练 |
| 模型发布 | 2024.7  | Llama 3.1                                                    | Meta                    | 8B, 70B, 405B参数，128k上下文长度                            |
| 模型发布 | 2024.9  | GPT-o1 (Omni-1)                                              | OpenAI                  | 强调深度推理和复杂问题解决，体现了OpenAI的重心向推理模型转移的趋势 |
| 模型发布 | 2024.12 | Sora 1.0                                                     | OpenAI                  | OpenAI正式向公众发布Sora 1.0                                 |
| 模型发布 | 2024.12 | Gemini 2.0 Flash                                             | Google                  | 强调效率和速度，使其成为大容量、低延迟任务的理想选择         |
| 模型发布 | 2025.1  | OpenAI Operator                                              | OpenAI                  | OpenAI的AI Agent，可以使用自己的浏览器在网络上自主执行任务，允许其键入、点击和滚动 |
| 模型发布 | 2025.1  | DeepSeek-R1                                                  | DeepSeek                | 671B参数，开源MoE模型，中国团队的推理模型首次达到世界领先水平，且距离OpenAI o1模型发布仅仅过去大约4个月，其API价格（Input成本USD 0.55/M tokens，Output成本USD 2.19/M tokens）远低于OpenAI推理模型o1（Input成本USD 15/M tokens，Output成本USD 60/M tokens） |
| 模型发布 | 2025.2  | GPT-4.5                                                      | OpenAI                  | 非推理模型，旨在更好地理解人类的意图、情感和微妙的暗示，使交互感觉更人性化、更直观。OpenAI 强调了它的"情商"（EQ）和适应用户语气的能力。 |
| 模型发布 | 2025.2  | Claude 3.7 Sonnet                                            | Antrhopic               | 专注于提高推理能力和编码能力                                 |
| 模型发布 | 2025.3  | Gemini 2.5 Pro                                               | Google                  | 推出Deep Research模式，采用CoT                               |
| 模型发布 | 2025.4  | Llama 4                                                      | Meta                    | 109B, 400B, 2T参数，混合专家架构、多模态（文本/图像输入）以及其最大（2T 参数）模型中的 1000 万个标记上下文窗口，使用NVIDIA H100/B100训练 |
| 模型发布 | 2025.4  | GPT-o3 and GPT-o4-mini                                       | OpenAI                  | "mini"版本表明OpenAI将推动更高效、能在移动设备上运行的，或更低成本的型号 |

- 中国公司大语言模型公司的重要进展时间节点

| 类别     | 时间    | 事件                               | 发布组织 | 介绍&意义                                                    |
| -------- | ------- | ---------------------------------- | -------- | :----------------------------------------------------------- |
| 模型发布 | 2023.9  | Alibaba Qwen 1                     | 阿里巴巴 | 阿里巴巴首次进军LLM领域，这是一个主要用于自然语言理解和文本生成的基础大型语言模型，由Qwen-7B开源版本 |
| 模型发布 | 2023.9  | Alibaba Qwen-VL                    | 阿里巴巴 | 该系列将 Qwen 的功能扩展到多模态理解，可以处理和生成来自视觉和文本信息的内容 |
| 模型发布 | 2023.11 | DeepSeek-Coder                     | DeepSeek | DeepSeek 首次进军专业大语言模型领域，重点关注编码能力。该课程基于大量代码和自然语言语料库进行训练，并采用"中间填充"等功能来增强代码生成和填充能力 |
| 模型发布 | 2023.11 | DeepSeek-LLM                       | DeepSeek | 这是 DeepSeek 的通用 LLM 系列，以各种规模（例如 7B、67B 参数）发布了基础版和聊天版。旨在与 Llama 2 等通用 LLM 竞争 |
| 模型发布 | 2024.1  | DeepSeek-MoE                       | DeepSeek | DeepSeek 首次尝试混合专家 (MoE) 架构，探索其在可扩展性和效率方面的优势。已发布基础版和聊天版 |
| 模型发布 | 2024.2  | Qwen 1.5 Series                    | 阿里巴巴 | 这是 Qwen1.0 的更新系列，重点提升了性能并扩展了参数大小范围（例如 1.8B、4B、7B、14B、32B、72B、110B）。该系列中的许多模型都是开源的 |
| 模型发布 | 2024.3  | Qwen-MoE                           | 阿里巴巴 | 在 Qwen 系列中引入混合专家 (MoE) 架构，允许更有效地缩放参数，同时保持或提高性能 |
| 模型发布 | 2024.4  | DeepSeek-Math                      | DeepSeek | 专注于数学推理，由 DeepSeek-Coder-Base 初始化。它引入了组相对策略优化 (GRPO) 等技术，以突破数学问题解决的极限 |
| 模型发布 | 2024.5  | DeepSeek-V2                        | DeepSeek | 这是一个重要的里程碑，它将混合专家 (MoE) 架构与多头潜在注意力 (MLA) 相结合，通过显著压缩键值缓存来提升推理效率。DeepSeek-V2 总共拥有 236B 参数，其中 21B 为激活参数。DeepSeek-Coder-V2 在 V2 的基础上构建，拥有更强大的编码能力，支持 338 种编程语言和 128K 上下文窗口 |
| 模型发布 | 2024.6  | Qwen 2 Series                      | 阿里巴巴 | 一套全面的基础和指令调优语言模型，涵盖从 0.5B 到 72B 的更广泛参数范围，包括密集模型和 MoE 模型。扩展了多语言支持 |
| 模型发布 | 2024.8  | Qwen-Audio                         | 阿里巴巴 | 专为音频相关任务而设计，能够处理和理解各种音频输入（人类语音、自然声音、音乐、歌曲）并生成文本输出 |
| 模型发布 | 2024.9  | Qwen2.5 Series                     | 阿里巴巴 | 在 Qwen2 的基础上进行了进一步的增强，引入了更多模型大小（例如 3B、14B、32B）并提升了性能。此外，还推出了 Qwen2.5-Coder 和 Qwen2.5-Math 等专用版本 |
| 模型发布 | 2024.9  | Doubao-PixelDance & Doubao-Seaweed | 字节跳动 | 这是字节跳动进军文本转视频生成领域的重要一步，旨在与 OpenAI 的 Sora 等模型相媲美 |
| 模型发布 | 2024.10 | Doubao LLM & Chatbot               | 字节跳动 | 这标志着字节跳动正式进军中国大型语言模型聊天机器人领域。该平台涵盖了用于对话、文本生成、摘要和基础理解的通用语言模型 (LLM) |
| 模型发布 | 2024.12 | DeepSeek-V3                        | DeepSeek | 规模和效率实现重大飞跃，共计 671B 参数，每个 token 激活 37B。进一步优化了 MoE 和 MLA 架构，并采用了先进的训练稳定性和效率技术（例如动态低秩投影、自适应查询压缩、联合键值存储）。已在 14.8 万亿 token 上进行训练 |
| 模型发布 | 2024.12 | Doubao Visual Language Model       | 字节跳动 | 该模型将强大的视觉理解能力与语言能力相结合。它可以识别图像中的复杂内容（对象、关系、文化背景），基于视觉信息进行推理（例如，根据图片解决数学问题、分析学术论文中的图表），并提供细致入微的视觉描述。其目标是达到与 GPT-4o 相当的能力。 |
| 模型发布 | 2025.1  | DeepSeek-R1                        | DeepSeek | 这是一个专为高级推理能力而设计的模型，利用大规模强化学习。目标是在复杂的数学、代码和科学推理任务上达到与 OpenAI o1 模型相当的性能。基于 DeepSeek-V3-Base 进行初始化 |
| 模型发布 | 2025.1  | Doubao 1.5 Pro                     | 字节跳动 | 这是一个功能强大的模型，引入了"深度思考"模式，旨在进行高级推理。它采用混合专家 (MoE) 架构，总共包含 200B参数，每次推理仅需 20B激活参数，从而显著降低了成本。 |
| 模型发布 | 2025.1  | Doubao Realtime Voice Model        | 字节跳动 | 用于端到端语音对话的集成语音理解和生成模型。它旨在实现高度富有表现力和情感感知的语音，超越传统的级联方法 |
| 模型发布 | 2025.3  | QwQ                                | 阿里巴巴 | 一个专门为提升人工智能推理能力而设计的实验研究模型，类似于 OpenAI 的 o1 模型。它专注于复杂的多步骤推理 |
| 模型发布 | 2025.3  | Qwen2.5-Omni                       | 阿里巴巴 | 多模态性方面的重大进步，可接受文本、图像、视频和音频作为输入，并生成文本和音频作为输出。这实现了类似于 OpenAI 的 GPT-4o 的实时语音聊天 |
| 模型发布 | 2025.4  | Qwen 3 Series                      | 阿里巴巴 | 最新旗舰系列，融合了密集架构和 MoE 架构（6 B-235B参数）。一项关键创新在于统一了"思考模式"和"非思考模式"，并采用思考预算机制，允许根据查询复杂度动态切换模式。多语言支持扩展至 119 种语言。在多项基准测试中均取得最佳结果 |
| 模型发布 | 2025.5  | Seed1.5-VL                         | 字节跳动 | 这是字节跳动的旗舰多模态模型，具有强大的通用视觉理解和推理能力，在众多公开的 VLM 基准测试中均取得了最佳性能。此外，该模型还显著降低了推理成本 |

#### 1.1.1.2 大语言模型发展趋势

- AI正在逐步超越人类
  - 在数学能力、阅读理解推理能力、科学问题回答方面，AI模型已经在诸多测试中超越了人类的表现。这将带来持续且深远的社会变革、挑战以及担忧。

    <img src="{{ '/image/1_1_1/ai_vs_human.png' | relative_url }}" style="zoom: 33%;" />

- 开源与闭源大模型性能差距缩窄
  - 详细模型表现对比记录在该文件中：[LLM模型表现差异]({{ '/appendix/llm_performance_tracking.xlsx' | relative_url }})

    <img src="{{ '/image/1_1_1/close_vs_open_1.png' | relative_url }}" style="zoom: 33%;" />

    <img src="{{ '/image/1_1_1/close_vs_open_2.png' | relative_url }}" style="zoom: 33%;" />

  - 开源模型领域，国外的主要玩家是Meta的Llama模型系列
    - 2022年11月GPT-3.5发布，2023年7月Meta发布的llama-2-70b模型在语言和推理能力上接近GPT-3.5，在代码能力上显著落后于GPT-3.5，时间差为8个月。
    - 2023年3月GPT-4发布，2024年4月Meta发布的llama-3-70b模型在语言和推理能力上接近GPT-4，在代码能力上追平GPT-3.5，时间差为1年。
    - 2024年5月GPT-4o发布，2024年7月Meta发布的llama-3.1-405b模型在语言、推理能力、代码能力上追平GPT-4o，在部分测试集表现超过GPT-4o，时间差为2个月。
    - 2024年12于OpenAI o1推理模型发布，2025年4月Meta发布的llama-4-maverick-400b在语言、推理能力上接o1，时间差为4个月。

- 中国与美国大模型性能差距缩窄
  - 详细模型表现对比记录在该文件中：[LLM模型表现差异]({{ '/appendix/llm_performance_tracking.xlsx' | relative_url }})

    <img src="{{ '/image/1_1_1/china_vs_us_1.png' | relative_url }}" style="zoom: 33%;" />

    <img src="{{ '/image/1_1_1/china_vs_us_2.png' | relative_url }}" style="zoom:33%;" />

  - OpenAI的模型在2022.12-2024.5期间内领先于中国玩家一个较为明显的身位，此后差距迅速缩窄
    - 2022年11月GPT-3.5发布，2023年11月阿里巴巴qwen-72b模型在语言和推理能力上接近GPT-3.5；2024年1月DeepSeek发布的deepseek-llm-67b语言、推理和代码能力接近GPT-3.5。模型表现追平时间差为1年左右。
    - 2023年3月GPT-4发布，2024年5月DeepSeek发布的deepseek-v2在语言和推理能力上初步接近GPT-4，代码能力与GPT-4仍有差距；2024年6月，阿里巴巴qwen-2-72b在语言和推理能力上初步接近GPT-4，代码能力与GPT-4持平。模型表现追平时间差为1年左右。
    - 2024年5月GPT-4o发布，2024年12月DeepSeek发布的deepseek-v3在语言和推理能力上与GPT-4o表现接近，在部分代码和数学测试集中超过GPT-4o水平；2025年1月阿里巴巴发布的qwen-2.5-72b在语言和推理能力上与GPT-4o表现接近，在代码和数学测试集中与GPT-4o仍有差距。模型表现追平时间差为7个月左右。
    - 2024年12月OpenAI o1推理模型发布，2025年1月DeepSeek发布的deepseek-r1在语言、推理、数学、代码能力上全面逼近o1在部分测试集中表现超越o1；2025年4月，阿里巴巴发布的qwen3-235b-a22b在语言、推理、数学、代码能力上全面逼近o1。模型表现追平时间差缩短至1个月左右，即使计算OpenAI o1 preview于2024年9月发布这一时间差，追平时间差也至多为4个月。

- 头部玩家的竞争格局
  - 头部大语言模型的表现

    <img src="{{ '/image/1_1_1/top_ai_models.png' | relative_url }}" style="zoom:33%;" />

  - 2023年3月OpenAI发布GPT-4，Anthropic在2023年7月发布的claude-2在语言理解和推理能力上接近GPT-4；Google在2023年12月发布的gemini-1-ultra在语言理解和推理、编程能力上接近GPT-4；Anthropic在2024年2月发布的claude-3-opus在语言理解和推理能力、编程能力上超越GPT-4；Google在2024年2月发布的gemini-1.5-pro在语言理解和推理能力、编程能力上部分追平GPT-4。时间差为3-8个月。
  - 2024年5月OpenAI发布GPT-4o，Anthropic在2024年10月发布的claude-3.5-sonnet，以及Google在2024年12月发布的gemini-2.0-flash在各项性能表现上反超GPT-4o。时间差为5-7个月。
  - 2024年12月OpenAI发布o1，Anthropic在2025年2月发布的claude-3.7-sonnet在语言理解和推理能力上接近01，在编程能力上超越o1。时间差为2个月。
  - 2025年3月OpenAI发布o3，Google在2025年5月发布的gemini-2.5-pro全面超越o1，与o3性能齐平；Anthropi在2025年5月发布的claude-4-sonnet在代码能力上领先03。时间差为2个月。

> [!IMPORTANT]
> Take Away
> - 模型性能方面，2024年下半年开始，OpenAI对Google/Anthropic的领先优势保持区间从6个月缩短至2个月，开源玩家如DeepSeek/Alibaba/Meta对OpenAI的优势也发起了冲击，OpenAI对中国及开源AI企业的领先保持区间缩短至4个月以内，AI大模型进入群雄争霸时代。
> - deepseek-r1-0529版本在文字理解和推理、代码能力上追平o3/gemini-2.5-pro/claude-4-sonnet水平。值得注意的是，deepseek-r1-0529在用于检验模型幻觉的SimpleQA测试集中的表现与o3/gemini-2.5-pro仍有较大的差距，三者在该测试集上的表现依次为27.8%/49.4%/50.8%。


### 1.1.2 Scaling Law

#### 1.1.2.1 What is Scaling law?

- Scaling law是OpenAI在2020年发布的文章 "[Scaling Laws for Neural Language Models]({{ '/reference/1_1_2/scaling_laws_for_neural_language_models.pdf' | relative_url }})"中提出，文章通过实验研究了影响Transformer架构的语言模型表现的因素。
- 模型表现（Test Loss，T，具体是用cross-entropy loss这一指标衡量）与模型大小（Parameters, N）、数据集大小（Data Size, D）以及训练所需的计算量（Compute, C）之间存在幂律（Power-law）关系。当不受其他两个因素影响时，模型表现与三个要素 N、D、C 均呈幂律关系，如下图所示，趋势跨越六个数量级以上。模型表现与模型的架构参数（深度和宽度）关系不大。
  - 计算量/算力消耗量一般用FLOP（FLoating-point OPeration）为单位衡量
  ![]({{ '/image/1_1_2/scaling_law.png' | relative_url }})
- 为了获得最佳模型性能，须同时扩展N, D, C三个要素。只要同时扩展 N 和 D，模型表现就能可预测地提升；但如果 N 或 D 保持不变，而另一个增加，就会进入收益递减状态。性能损失可预测地取决于比率 $N^{0.74}/D$，这意味着每当将模型大小增加 8 倍时，只需将数据量增加大约 5 倍即可避免损失。
- 训练曲线（x轴一般是训练迭代次数/训练消耗的算力，y轴为模型表现）遵循可预测的幂律，幂函数的参数大致与模型大小无关。通过推断训练曲线的早期部分，可以粗略地预测如果训练时间更长，模型表现会是多少。
- 大型模型比小型模型具有更高的样本效率，能够以更少的优化步骤和更少的数据点达到相同的性能水平。

  <img src="{{ '/image/1_1_2/training_efficiency_1.png' | relative_url }}" style="zoom:80%;" />

  ![]({{ '/image/1_1_2/training_efficiency_2.png' | relative_url }})

- 当计算量 C 固定，模型规模 N 或可用数据 D 不受任何其他限制时，我们可以通过训练非常大的模型，并在远未达到收敛时停止训练，从而获得最佳模型性能。因此，采取上述训练策略时其训练数据利用效率将远高于基于训练小型模型直至收敛的策略，数据需求随着算力增加而增长的速度非常缓慢$D ∼ C^{0.27}$ 。

  <img src="{{ '/image/1_1_2/compute_allocation_strategy.png' | relative_url }}" style="zoom: 80%;" />

#### 1.1.2.2 Why Now?

- Scaling前置条件的成熟（1985-2017）
  - 巨量的数据
    - 多层神经网络（深度学习）的架构在1980-1990年代就已经被提出了，并用于图像识别等任务。神经网络架构需要很多的数据来训练，但当时只有非常少的领域（如手写识别、字母识别、语音识别）有足够数量的优质数据，此外当时的计算机非常昂贵，对这一项技术的兴趣在1990年代中期就消退了。
    - 大数据驱动的模型始于Fei-Fei Li于2009年在计算机视觉领域提出的ImageNet数据集，2012年基于该数据集训练的AlexNet标志着深度学习的一个里程碑式成就，人们开始意识到数据和深度神经网络架构的力量。
    - 互联网的出现是让"巨量的数据"成为可能的必要前置条件。
  - 高效且便宜的算力
    - GPU的出现是一个重要的转折点，CPU的"线性计算"转向GPU的"并行计算"。并行计算大幅加快了计算机的运算速度，让处理大量数据所需的时间大幅缩短，基于大数据训练的大模型成为可能。
    - 更多相关内容请见章节[1.1.6](#1.1.6 CPU and GPU)
  - 可以充分利用并行计算和巨量数据的算法结构
    - 神经网络是一种非常便于scaling的结构，只需要堆叠更多的神经元，叠加更多层网络就可以实现模型参数的scale up。
    - Transformer之前的RNN模型是"线性序列（Sequential）"模型，其scaling能力受限。Transformer模型是一个适合scaling的模型，它可以充分利用GPU的并行计算能力和互联网上的巨量数据。
    - 更多相关内容请见章节[1.1.3](#1.1.3 Transformer)

- Transformer开启的预训练时代（2017-2023）：从论文到产品
  - 需要回答的问题是："Attention is All You Need"于2017年7月发布，震惊世界的ChatGPT产品于2022年11月发布，这其中经过了5年的时间，发生了什么？
  - 规模扩张
    - Sclaing Law的确立：2020年OpenAI论文正式提出了Scaling Law，这给研究者们提供了一个明确的方向和信心。
    - 参数扩张：Scaling Law是大语言模型性能提升的关键，在模型参数足够大之前，其性能并不足以处理复杂的任务，表现并不足以满足用户的需求。
    - 算力扩张：更高性能的GPU、以及更大的算力集群（云计算基础设施建设）。此外，科研机构一般没有那么充足的预算去建立大规模的算力集群，这也导致了基于"Scaling Law"不断扩大模型参数量、算力规模提升表现的基础大模型的主要创新集中在企业中。
    - 数据扩张：训练更大参数量的模型需要有更大的数据量的支持，数据整理、清洗、整理需要很多时间。
    - 工程能力：将模型部署在巨大数量的算力集群上进行同步/并行训练的能力，对大参数量模型和大量数据进行高效处理和运算的能力是需要解决的工程挑战。
  - 训练方法的完善
    - GPT decoder-only模型的提出
    - 对Scaling Law的理解
    - RLHF等微调方式的确立
  - 面向公众的商业化调整（将技术打磨成产品）
    - 足够高效和经济的推理能力：能够接受大量用户的同时访问，足够低的成本以扩大受众范围。
    - 安全性：设立围栏和过滤器，确保不合适的内容不会向用户呈现。
    - 产品形态：OpenAI以Chatbot形式推出产品是其快速取得成功的关键之一，这一产品形态符合用户的自然交互习惯。
    - 用户需求的解决：长时间有粘性的用户群体和付费意愿建立在产品本身能够为用户带来的价值上，大语言模型在接入互联网之后提供足够准确的信息，生成足够合理的分析和推理结果，能够切实协助用户解决生活和工作中的问题。

#### 1.1.2.3 Scaling Law的实践验证

- 根据斯坦福发布的[AI Index Report 2025]({{ '/reference/hai_ai_index_report_2025.pdf' | relative_url }})（详见[报告数据源](https://drive.google.com/drive/folders/1AxxxL9-AsaeMdDKtTNHCR1KqEJTsHCod)），大模型继续在参数、数据量、算力三个方向Scale Up。

- **模型参数量**：2024年，模型参数量的增长速度稍有放缓，与2023年基本持平，未见明显级数增长，值得留意的是，Meta计划将于2025年推出2T参数的MoE架构Llama 4 Behemoth，激活参数288B。

  <img src="{{ '/image/1_1_2/parameter_scaling_1.png' | relative_url }}" style="zoom:33%;" />

  <img src="{{ '/image/1_1_2/parameter_scaling_2.png' | relative_url }}" style="zoom:33%;" />

- **训练数据集**：前沿大模型训练数据集的规模大约每八个月翻一倍。2024年夏季，Meta发布的Llama 3.3模型的训练数据量达到15万亿个token。

  <img src="{{ '/image/1_1_2/data_scaling_1.png' | relative_url }}" style="zoom:33%;" />

  <img src="{{ '/image/1_1_2/data_scaling_2.png' | relative_url }}" style="zoom:33%;" />

- **训练算力消耗**：前沿人工智能模型的训练计算量大约每五个月翻一倍，训练所消耗的能源大约每年翻一倍。训练算力消耗排名前 10 的中国语言模型自 2021 年底以来每年消耗的算力约扩大3 倍，远低于 2018 年以来世界其他地区每年 扩大5 倍的速度。

  <img src="{{ '/image/1_1_2/compute_scaling_1.png' | relative_url }}" style="zoom:33%;" />

  <img src="{{ '/image/1_1_2/compute_scaling_2.png' | relative_url }}" style="zoom:33%;" />

- **硬件GPU/TPU**：用于训练模型的GPU性能也在逐年提升，算力成本在逐渐下降。根据估算，硬件成本每年下降 30%，这使得人工智能训练越来越经济实惠。

  <img src="{{ '/image/1_1_2/hardware_scaling_1.png' | relative_url }}" style="zoom:33%;" />

  <img src="{{ '/image/1_1_2/hardware_scaling_2.png' | relative_url }}" style="zoom:33%;" />

#### 1.1.2.4 What Now?

- 根据[Geoffrey Hinton于2025年2月的讲座](https://www.bilibili.com/video/BV1iBoSYJEU8/?share_source=copy_web&vd_source=e339681838b10234d37810391dea4e4b&t=1100)，单纯的scaling带来的收益正在递减，当训练算力、训练数据翻倍时，模型性能的提升可能非常微小，需要新的技术和工程的创新来将模型性能进一步提升。现在使用的MoE架构就是通过工程上的创新来提升模型性能的方案。
  - 一个可以参考的例子是Moore's Law，即"集成电路（IC）上的晶体管数量大约每两年翻一番"。人们对于Moore's Law是否已经终结有过诸多讨论，基于CPU的Moore's Law可能确实在2010年左右就已经停滞了，但是基于并行计算的GPU的出现，集成电路的性能又按照Moore's Law继续提升了10多年。

    <img src="{{ '/image/1_1_2/moores_law.png' | relative_url }}" style="zoom: 25%;" />

- Scaling需要的更多的数据有可能会变成瓶颈，如果想要达到过去几年这样10倍、100倍的scaling，互联网上已经没有足够的数据了。探究新的数据获取方法是一个重要的课题。更多关于数据瓶颈内容请见[章节1.2.2](#1.2.2 数据是模型发展的"石油"——如果"石油"被用完了怎么办？)。
- 更多的模型参数参数、训练数据、训练算力消耗意味着更贵的模型训练成本，2024年前沿模型的训练费用也持续增长。没有足够强大且可持续的资本支撑的创业公司，在基础模型的竞争中已经逐步乏力并且被拉开差距。

  <img src="{{ '/image/1_1_2/training_cost_scaling_1.png' | relative_url }}" style="zoom:33%;" />

  <img src="{{ '/image/1_1_2/training_cost_scaling_2.png' | relative_url }}" style="zoom:33%;" />

  <img src="{{ '/image/1_1_2/training_cost_scaling_3.png' | relative_url }}" style="zoom:33%;" />

- Scaling红利期的结束：[LLM六小龙](https://36kr.com/p/3316013463266566)
  - LLM六小龙的机会可能是因为当时国内的大模型发展还处在scaling的红利期，scaling方向明确，且处于scaling早期，scaling规模初创企业尚能处理，scaling回报比较丰厚。但随着红利期的结束，现在可能进入了收益递减期，scaling的回报下降，性能增长速度减缓，需要有持续资金支持、能够接受递减收益并依然耐心持续投入、有真正技术和工程创新能力的企业才能继续在市场上占据一席之地。一个直观但并不准确的趋势描述如下图所示。

    <div class="image-container">
        <img src="{{ '/image/1_1_2/diminishing_return_to_scale.png' | relative_url }}" style="zoom: 40%;" />
    </div>

  - 有行业知名大佬或顶尖人才带队，技术团队能力出众，首轮融资就突破为独角兽级别，以及要争夺国产OpenAI的位置，这些都是跻身"AI六小龙"的准入门槛。
  - 零一万物（2023年5月创立）
    - 将超大模型交给了阿里训练，明确不再追逐AGI，放弃预训练转向应用。零一万物辛辛苦苦拉起的技术团队分崩离析。包括曹大鹏、戴宗宏等负责核心技术和产品方向的高管接连出走，最近一次是模型预训练负责人谷雪梅宣布离职。曾是零一万物核心成员的黄文灏加入了字节，而最近前零一万物联创谷雪梅被爆离职，正在筹备创业。
    - 根据《智能涌现》报道，2024年12月底，零一万物的预训练团队收到阿里"通义"的offer，Infra团队则收到了阿里云团队的offer。"阿里收编"的背后，源自零一万物的判断：初创公司投入超大模型预训练的性价比，太低了。
    - 作为六小龙中最早掉队的零一万物的创始人，李开复3月曾直接回应，中国市场最终可能只有三家能够真正站稳脚跟的大模型提供商，分别是DeepSeek、阿里巴巴和字节跳动。
  - 百川智能
    - 专注医疗垂类赛道，在字节、阿里、腾讯等大厂争相上新基础模型时，其创始人王小川曾提出百川智能的底层模型将对标OpenAI，但如今其基础大模型进入了静默期，不再更新。
    - 2025年3月，作为王小川在搜狗时期的老部下，在百川智能负责大语言模型技术开发的联创陈炜鹏宣布离职，同一时期离开的还有另一位联创焦可。
  - 智谱AI（2023年4月成立）
    - 基础大模型最后一次更新停止在2024年12月发布深度推理模型GLM-Zero-Preview。进入2025年，智谱的新动作只有发布了开源的GLM-4-32B-0414系列模型。智谱AI原视频模型负责人加入字节。
  - 月之暗面（2023年4月成立）
    - 2025年1月20号发布的Kimi1.5推理模型热度被 DeepSeek R1 压过，随后再无更新。月之暗面前大模型产品负责人独立创业。
  - 跃阶星辰（2023年4月成立）
    - 2025年1月一周内集中更新6款模型后，再无新迭代。
  - MiniMax（2021年12月成立）
    - 2025年5月更新的MiniMax Speech - 02选择的也是文本转语音的场景，唯有继续推进基础模型。
  - 从2024年下半年开始，智谱之外，它们也几乎再无融资消息传出，随着字节、阿里、腾讯今年积极布局AI，拿出动辄百亿的资金扶持自家的AI APP，美团、小红书等也都在自建大模型团队，六小龙的先发优势已经被完全赶超。
  - 在去年开源与闭源孰优孰略的争议中，杨植麟曾经在采访中表示"开源落后于闭源"，李彦宏也曾肯定地表示，闭源模型将持续保持对开源模型的优势，商业化闭源模型一定能在市场中胜出。彼时，六小龙们还可以凭借赶超开源模型一步的技术优势，输出自己的美好故事。但当免费开源且技术更为强大的DeepSeek出现后，开源成了新潮流，六小龙最后的堡垒也迎来炮击。
  - 曾在AI 1.0时代被称为AI四小龙的商汤、旷视、云从、依图，殷鉴在前。2016年，AlphaGo横空出世带火了AI，那时技术领先的上述四家创业公司被并称为"AI四小龙"。其中商汤于2021年港股上市，云从在2022年成功科创板上市。但高研发投入之下，商业化却迎来惨淡局面。随着OpenAI出现，大模型成为AI主流，拿不到融资的四家公司，被迫开启大裁员。


> [!IMPORTANT]
> Take Away
> - 基础模型训练资金需求大，大部分初创企业以及科研机构难以在这场Scaling Up的资本竞争中击败大厂脱颖而出。大部分初创企业寻求AI在细分领域/场景的应用方面的商业机会更大。

> [!TIP]
> Todo
> - Scaling Law有上限吗？
>   - Scaling Law是可以无限scale up的吗？还是说在超过特定参数规模后，边际收益是递减的？
>   - MoE架构是否是因为Dense模型继Scale的收益在下降，所以针对不同细分领域/功能进行Scale up？本质上是在把上一波dense模型scale up的技术红利再"精细化的"吃一遍？
> - 为什么现在没人投资大模型了？
>   - 中国算力卡脖子问题什么时候能解决？
>   - 人才可能聚集在某些大厂里（DeepSeek，阿里，字节）？

### 1.1.3 Transformer

#### 1.1.3.1 一切的开端：神经网络（Neural Network）

- 人工神经网络是一种模仿人类大脑神经系统的机构和工作原理的算法。神经网络中的每一个单元，被称之为神经元（Neuron/Cell）。最核心的思想是，如果我们相信人工神经元和人类大脑神经元是差不多的东西，那么它会给我们信心，我们可以用非常大的人工神经网络去做人类能做到的事情（人类大概有86-100B神经元，这对应着100-1000T个神经元之间的连接；DeepSeek-R1模型有685B参数，对于简单的人工神经网络来说，每两个神经元之间的连接就对应了一个参数）。

  <img src="{{ '/image/1_1_3/human_neurons.png' | relative_url }}" style="zoom:40%;" />

  <img src="{{ '/image/1_1_3/artificial_neurons.png' | relative_url }}" style="zoom:20%;" />

- 每个神经元当中都会进行某种数学计算，其中数学计算涉及的参数（Parameters）也被称为权重（Weights）。

  <img src="{{ '/image/1_1_3/inside_neuron.png' | relative_url }}" style="zoom:20%;" />

- 将多个神经元连接起来，就会变成一个层（Layer），将多个层堆叠起来就会变成一个网络（Network）。在输入层和输出层中间的层被称为隐藏层（Hidden Layer），其工作是将输入的特征进行提取（概括/总结/提取），下图是一个示例，隐藏层将输入的价格、成本等信息抽象成可负担性、品牌认知等信息，最终输出一个概率预测。

  <img src="{{ '/image/1_1_3/selling_prediction_nn.png' | relative_url }}" style="zoom:20%;" />

  <img src="{{ '/image/1_1_3/feed_forward_nn.png' | relative_url }}" style="zoom:20%;" />

- 人工神经网络和人脑一样是一个"黑箱"（Black Box），我们虽然可以观察到每个神经元中的参数，但却无法解释其含义，在上述例子中的"可负担性、品牌认知"等特征只是我们猜测/想象出来的"含义"。神经网络的这一特征带来了一些挑战，即无法解释的东西是存在风险的，是无法被溯源和追溯责任的，在一些领域，如医疗、金融等看重可解释性、可问责性（Accountability）、可信度的领域，神经网络的使用存在较大限制。比如说，对于国内的银行信贷风险评估模型，业界会更倾向于使用可解释性更强，但预测准确性更弱的逻辑回归（Logistic Regression）模型，而非可解释性弱但预测准确性更高的神经网络模型。更多内容请见[章节1.3.5](#1.3.5 可解释的AI与安全的AI)。


> [!IMPORTANT]
> Take Away
> - 神经网络是一个"黑箱"，缺乏可解释性，该特点可能在一些透明度非常重要的领域限制其应用。
> - 我觉得限制黑箱模型在某些领域应用的原因最大的还是"责任由谁来承担的问题"，因为人类大脑也是黑箱，如果人工大脑可以做到比人类大脑更低的失误率，某种意义上它就是比人类更优秀的员工。

#### 1.1.3.2 Transformer出现之前：循环神经网络 (RNN)

- Recurrent Neural Network (RNN) 的表示方法
  - RNN在t=0时刻接受输入$x_0$后，会将信息传递给下一时刻的自己，即模型在t=1时刻接受输入$x_1$的同时，也会知道上一时刻的输入信息。比如下图中，只有一个RNN单元，右侧是按照时间顺序对同一个单元在不同时刻的状态进行了表示。

    <img src="{{ '/image/1_1_3/unrolled_rnn.png' | relative_url }}" style="zoom: 33%;" />

- [RNN的工作方式](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
  - Tokenization/Encoding/Embedding：把文字表示转化为数字表示
  - 将数字表示输入由神经元（Neuron/Cell）组成的中间层（Hidden Layer），每个神经元都包含了诸多参数（Parameter），整个网络输出的是概率，这个概率是对下一个token的预测（Confidence）
  - RNN的特殊之处在于，前序神经元会将信息传递给后续神经元，并将之前输入的信息都于下一个输出Token的预测（例如，模型需要能够做出正确预测hel->l, hell->o）
  - RNN的学习方法：把预测的结果和真实的结果即正确答案进行比较，并通过调整hidden layer参数的方式，在多次的训练（Epoch/Iteration）后，实现能够能够正确预测下一个token的目标，调整参数的方法是梯度回传（Gradient Backpropagation）

    <img src="{{ '/image/1_1_3/rnn_demo.png' | relative_url }}" style="zoom: 33%;" />

  - 想要提升模型的预测准确率，让网络变得更大是一个直观的方法，比如说让模型可以接受更长的输入，在预测时可以接触到更多的context。即需要更多neuron和更多参数，也就是模型层面的Scale Up。想要提升模型的泛化能力，就需要让模型见过更多的案例，即提供更多的训练数据供模型进行学习，就是数据层面的Scale Up。
  - 数据对模型训练结果的影响：模型预测的本质是预测概率，训练数据集的组成就会影响到训练的结果，因此高质量且均衡的训练数据非常重要。比如说，在预测单词的下一个字母的任务里，训练数据有transformer, translate, transfer三个词，在没有更多的语境支持的情况下，trans+f有两个词，trans+l有一个词，那可能模型就会预测trans->f，因为这是更大概率正确的答案。

- [RNN的Encoder-Decoder（编码器-解码器）架构](https://medium.com/analytics-vidhya/encoder-decoder-seq2seq-models-clearly-explained-c34186fbf49b)
  - 架构总览
    - Ilya Sutskever在2014年NeurIPS上发布论文"[Sequence to Sequence Learning with Neural Networks]({{ '/reference/1_1_3/seq_to_seq_nn.pdf' | relative_url }})"，提出了用于解决Sequence-to-Sequence任务的Encoder-Decoder模型（被称为Seq2Seq模型）。也是在这次会议上，Ilya提出了类似Scaling Law的观点，"如果有足够的数据和足够大的模型，成功是可以保证的"。

      <img src="{{ '/image/1_1_3/encoder_decoder_high_level.png' | relative_url }}" style="zoom:33%;" />

    - 下图中展示了Encoder和Decoder按时间顺序输入和输出的情况，每一个小长方形代表的是Encoder/Decoder在某一个时间的状态，并非表示有4个Encoder单元按顺序串联在一起。

      <img src="{{ '/image/1_1_3/seq2seq_by_time.png' | relative_url }}" style="zoom: 33%;" />

    - 自回归（Autoregressive）是一种使用其自身过去的输出作为输入来生成未来的输出的方法。Decoder部分通过使用自回归的方法，让模型输出长度可以与输入长度不一致（比如我们问GPT一个简短的问题，希望它能给出详细的大段回复，模型输入和输出的长度是不同的）。
    - Encoder-Decoder结构的高层设计理念在后续的Transformer模型、Figure AI的Helix模型中都有使用。
  - Encoder
    - Seq2Seq模型的Encoder是一个多层循环神经网络（使用的是 LSTM架构），它每次读取一个单词的输入序列（例如一个英文句子）。它的作用是将输入序列的全部含义压缩成一个固定大小的向量，称为上下文向量（Context Vector），也被称为"思维向量"（Thought Vector），这个向量代是文字输入的数值表示。
  - Context-Vector
    - Seq2Seq模型中使用的Context Vector和人形机器人公司Figure AI Helix模型的"大脑-小脑/快脑-慢脑"架构中用于链接大小脑的Latent Vector在抽象层面是类似的概念，都是输入信息的压缩表示。Context Vector和Latent Vector均采用复杂的高维输入（例如一个句子、一个视觉场景），并将其提炼为一个密集的数值向量以捕捉其基本含义，这种压缩的"理解"随后被用作生成输出序列的起点。当然，Helix的Latent Vector包含了更多模态的信息，包括视觉（来自摄像头）、音频（来自麦克风）、文本（来自用户命令）和本体感觉（机器人自身的关节位置和状态）等。

      ![]({{ '/image/1_1_3/figure_ai_helix.png' | relative_url }})

  - Decoder
    - 另一个多层 LSTM，使用来自encoder的context vector进行初始化。它的任务是基于初始上下文，逐个单词地生成输出序列（例如，将英文输入翻译成法语输出）。
  - 端到端模型
    - Seq2Seq 模型是端到端的，使用一个大型神经网络，直接将源文本翻译成目标文本。该模型只需在海量句对（原始语言-目标语言）数据集上进行训练，就能学习翻译所需的关系、语法和句法。这极大地简化了机器翻译模型的训练流程。
    - 判断模型是不是端到端的一个重要方法是看整个模型的梯度能不能回传（Gradient Backpropagation），目前基本上连接模型的不同模组的中间模块一定要是Vector形式才有可能实现梯度回传。
- RNN的局限
  - RNN不擅长处理长距离依赖关系（文段特别长，相关上下文距离特别远的时候）
    - 在 RNN 中，将token  A与远距离token Z 关联起来需要遍历所有中间token（B、C、...、Y），有很多中间的无关信息，阻碍了长距离信息的关联记忆。
  - RNN能够处理的输入内容长度有限
    - Seq2Seq模型中有固定长度的context vector，context vector在信息压缩（也叫特征提取，Feature Extraction）的过程中很有可能无法收集到所有重要的信息，限制了Encoder传输给Decoder的信息量，被称为信息瓶颈（Information Bottleneck）。这会导致信息遗漏和损失的问题，限制了RNN能够有效处理的输入内容的长度。
  - RNN的可扩展性有限
    - 梯度消失/爆炸问题：类比来说有点像"传话游戏"，一列人站成一排，第一个人要把一个词语用肢体语言传递给下一个人，队伍末尾的人再把这个词说出来，这个"按顺序"传递信息的过程中会丢失很多信息。同理，RNN在训练过程中，在梯度回传的时候，由于"队伍太长"会"丢失"很多信息，这限制了RNN的scale up能力。
  - RNN的内部运算不能并行，训练效率低
    - RNN具有顺序（线性）训练特性，步骤t 的计算依赖于步骤t-1的输出，这阻碍了单个序列的并行计算。这使得 RNN 训练速度缓慢且计算密集，并可能限制可用于训练这些模型的数据量（用大量数据训练花的时间太长了），同样限制了RNN的scale up能力。

#### 1.1.3.3 注意力机制

- RNN中的注意力机制
  - 注意力机制最早时于2014年由Yoshua Bengio等在论文"[Neural Machine Translation by Jointly Learning to Align and Translate]({{ '/reference/1_1_3/attention_rnn.pdf' | relative_url }})"中被提出，当时的目的是为了改进RNN的encoder-decoder架构中的context vector具有信息瓶颈的缺点。
  - 论文中提出的新方法是：每当模型在翻译过程中生成一个单词时，它都会搜索输入句中最相关的信息的位置。然后，该模型根据与这些相关信息的context vector以及所有先前生成的单词来预测下一个需要生成的单词。
  - 这种新方法与基础的encoder-decoder架构最重要的区别在于，它不会尝试将整个输入句子编码成单个固定长度的向量（即Seq2Seq方法里的context vector）。相反，它将输入句子编码成一系列向量（将往encoder中输入每个单词后的hidden state存储起来），并在解码翻译时自适应地搜索和选择这些向量的子集。

    <img src="{{ '/image/1_1_3/attention_rnn_structure.png' | relative_url }}" style="zoom: 30%;" />

  - 实现自动选择与下一个单词相关的内容的方法是加入了一个alignment model，它也是一个前馈神经网络。在整个encoder-decoder模型进行端到端训练的过程中，alignment model逐步学会识别如何选择相关内容，并且通过给予输入词语不同的"attention"权重，让模型在生成输出的时候选择性的关注重要的词语。下图展示了在翻译英文句子为法语句子的时候，在生成输出单词时对每个输入单词的关注度（黑色是0，白色是1）。

    <img src="{{ '/image/1_1_3/attention_rnn_example.png' | relative_url }}" style="zoom: 25%;" />

#### 1.1.3.4 Transformer模型

- Embedding and Positional Encoding
  - 与大多数 NLP 模型一样，Transformer 将输入标记（例如，单词或子词）转换为称为embedding的向量表示。
  - Transformer 的一个关键创新是位置编码（Positional Encoding）。由于模型架构本身不包含递归（RNN的架构）或卷积（CNN的架构），因此它没有固有的机制来理解序列中 token 的顺序或位置。为了解决这个问题，在编码器和解码器的输入embeddin中添加了位置编码。这些编码提供了每个 token 的相对或绝对位置的信息。

- 自注意力机制（Self-Attention）
  - Scaled Dot-Product Attention
    - 自注意力机制提供了将所有 token 位置直接关联起来的基本机制。自注意力机制使模型能够在处理每个单词时"权衡序列中不同单词的重要性"。对于给定的token，自注意力机制使其能够查看同一输入序列中的其他token，从而为其自身的表征收集上下文线索。

      <img src="{{ '/image/1_1_3/self_attention.png' | relative_url }}" style="zoom: 15%;" />

    - 该机制涉及三个可学习的权重矩阵，用于将输入embedding（或前一层的输出）投影到Query (Q)、Key (K) 和Value (V) 向量中。这三个权重矩阵是随机初始化的，并在 Transformer 模型的训练过程中通过反向传播进行学习，就像神经网络中的其他权重一样。
      - Query (Q)：当前正在处理的单词的表示。
      - Key (K)：序列中所有单词的表示，用于和Query进行相关性评分，从而确定相关性高的内容。
      - Value (V)：序列中所有单词的另一种表示，包含输入内容的完整信息，参与生成输出。
      - Q, K, V 均来自同一来源：编码器（或解码器）中前一层的输出。
      - 下图是一个示例，最左侧的是输入自注意力层的embedding，最右侧的是当前自注意力层的输出。

        ![]({{ '/image/1_1_3/qkv_calculation.png' | relative_url }})

  - 多头注意力机制（Multi-Head Attention）
    - 多头注意力机制允许模型同时从不同的表征子空间中捕捉不同类型的关系，从而丰富了学习到的表征。这意味着不同的头可以学习关注不同类型的关系（例如，句法、语义、指代关系）或输入的不同方面，从而增强模型捕获复杂依赖关系的能力。

    <img src="{{ '/image/1_1_3/multi_head_attention.png' | relative_url }}" style="zoom:50%;" />

- 编码器（Encoder）-解码器（Decoder）结构
  - 解码器获取编码器的输出，并以自回归的方式，一次一个单词地生成输出序列

    <img src="{{ '/image/1_1_3/transformer.png' | relative_url }}" style="zoom:40%;" />

  - Masked Self- Attention
    - 在解码器中引入"掩蔽"（Masked）自注意力机制是一项极具洞察力的改进。序列生成任务本质上是自回归的，这意味着对当前标记的预测取决于之前生成的标记。掩蔽机制巧妙地将注意力限制在目标序列中的先前位置，防止训练期间关注输出序列中的后续位置。
  - Encoder-Decoder Attention (Cross-Attention)
    - 解码器的第二个注意力子层对编码器的输出执行多头注意力机制。在这种"交叉注意力"机制中，Query (Q) 来自解码器中前一个带掩码的自注意力子层的输出，而Key (K) 和Value  (V) 来自编码器的最终输出。这使得解码器中的每个位置都能关注输入序列中的所有位置，从而有效地将源序列中的相关信息整合到目标序列生成过程中。

- Transformer是如何解决RNN遇到的挑战的？
  - 长距离依赖关系
    - 自注意力（Self-attention）机制允许序列中的任何 token 直接与任何其他 token 交互，无论它们之间的距离有多远。这为序列中任意两个位置之间的信息和梯度流创建了直接、短的路径，较短的路径长度可以最大限度地减少信号衰减，这对于捕捉长距离依赖关系（例如，能够引用更复杂的上下文）至关重要。
  - 可以处理比RNN更长的输入内容
    - Transformer可以处理可变长度序列，通常通过将较短的序列填充到训练期间定义的固定最大长度（上下文窗口）来实现。在此窗口内，自注意力机制可以有效地对依赖关系进行建模。
  - 易于扩展的结构
    - Transformer架构由重复的注意力模块和前馈层组成，并结合了残差连接和层归一化的有效利用，因此高度易于扩展。这使得研究人员能够训练具有数十亿参数的超深网络和模型（例如，大型语言模型），这些模型可以从海量数据集中学习。
  - 可并行计算
    - Transformer同时处理输入序列中的所有 token。自注意力机制下，所有 token 对之间的注意力得分可以通过高度优化的矩阵乘法运算在很大程度上独立计算。这种设计充分利用了 GPU 和 TPU 等现代并行处理硬件，从而显著缩短了训练时间，尤其是在大型数据集上优化明显。

- Transformer遇到的新挑战与解决方案
  - 由于自回归特性，随着输出文段长度的不断延长，dense attention的计算量会逐渐加大

    <img src="{{ '/image/1_1_3/dense_attention.png' | relative_url }}" style="zoom: 25%;" />

  - 相比RNN会需求更多的存储空间和算力

    <img src="{{ '/image/1_1_3/attention_vs_rnn.png' | relative_url }}" style="zoom: 13%;" />

  - 解决方案：Dense-attention v.s. Sparse-attention
    - Global-attention v.s. Local-attention
- Transformer的可解释性提升
  - 虽然深度学习模型常常被批评为"黑匣子"，但Transformers 中的注意力机制提供了一定程度的可解释性。模型计算的注意力权重表明在生成特定输出或表示时，对输入序列不同部分的关注程度。这些权重可以可视化，从而深入了解模型的决策过程，并突出显示哪些标记对其他标记的表示或特定预测的贡献最大。
  - 例如，谷歌人工智能展示了如何通过可视化注意力模式来揭示模型解决共指的能力，即通过展示代词"it"在不同语境下关注的是哪个名词。这为理解模型行为、调试以及改进模型设计提供了一个宝贵的工具。


> [!TIP]
> Todo
>
> - 在图像生成领域的重要模型Diffusion Model，也可以应用于语言生成、具身智能的VLA模型、World Model架构中
>   - Gemini Diffusion
>   - Generative Adversarial Network

### 1.1.4 如何训练一个大语言模型

#### 1.1.4.1 从Transformer到GPT

- Generative Pretrained Transformer (GPT)：只留下了Decoder的Transformer

#### 1.1.4.2 强化学习

- 强化学习（Reinforcement Learning, RL）
  - 强化学习于1980年代被提出，其算法的启发源自于人类婴幼儿的学习方式，即通过不断试错（Trial and Error）并根据反馈做出调整。心理学的研究中的Law of Effect表明，“产生令人满意的结果的行为往往会被重复，而产生不愉快结果的行为则不太可能被重复”，强化学习中关于奖励函数设计的底层逻辑正是基于此。
    - 以宠物狗的训练为例子，我们不会给它一本教科书，教它“如何坐下”。我们只会说“坐下”，等它终于坐下后，再给它零食。零食就是奖励。这种积极的反馈会强化狗狗的动作，让它下次更有可能坐下。
  - 强化学习中几个重要的概念包括：行动主体（Agent）、环境（Environment）、状态（State）、行动（Action）、奖励（Reward）
    - 打个比方，婴儿（Agent）看到一个色彩鲜艳的玩具（状态）。他们会伸手去抓它（行动）。玩具发出有趣的咔哒声（正向奖励）。这种正向反馈强化了婴儿的这个动作，使婴儿更有可能在将来再次抓取玩具。如果他们触碰到尖锐的东西（动作），他们会感到疼痛（负向奖励/惩罚），这会降低他们再次执行该动作的可能性。
  - 强化学习的流程
    - 需要设计一个策略/动作执行人（AI Agent，代理人），并将其置于一个特定的“环境”中，并赋予其明确的目标。学习的过程是一个简单的循环：
      - Agent执行一个操作（Action）。（例如，交易算法卖出一只股票）。
      - 环境（Environment）发生变化，并给予奖励（Reward）或惩罚。（例如，卖出产生了利润->奖励。卖出产生了亏损->惩罚）。
      - 代理利用这些反馈来更新其策略（Policy），从而变得更加智能。

        <img src="{{ '/image/1_1_4/rl_cycle.png' | relative_url }}" style="zoom:20%;" />

    - 它重复这个循环数百万次，有时甚至数十亿次。它通过纯粹的反复试错来学习，并遵循一个目标：在长期内最大化总奖励。它的目标不仅仅是下一次奖励，而是在它的整个生命周期内尽可能多地获得奖励。
- Tabular Quality Learning
  - 基于“策略表格”的强化学习方法，最终的目的是习得一套可以用表格表示的，可用于达成目标的策略（Policy）。
  - 我们可以举一个Frozen Lake游戏的例子。
    - 如下图所示，一个机器人在从起点S出发，要到终点G去，路上会经过一个结冰的湖面，F表示安全的冰面，H表示会掉入冰面的洞中。机器人总共有4种可以选择的行动（Action），即向上下左右四个方向移动一格，而整个环境共有16个状态（State），机器人处在每一个格子里的时候，都可以算做一种状态。此外我们还需要定义奖励函数，在这个游戏当中，到达终点G就可以获得奖励，调到洞里就会受到惩罚，在冰面上行走不会有奖励也不会有惩罚。

      <img src="{{ '/image/1_1_4/frozen_lake_game.png' | relative_url }}" style="zoom:20%;" />

    - 所有可能的行动和可能的状态一起构成了Q Table，其中的信息可以告诉我们机器人在每一个格子里的时候，应该如何行动（类似一个导航地图）。机器人通过不断尝试（比如随机进行4个方向的移动），积累经验（调整Q Table中的数值），最终找到一条可以稳定到达终点的行进线路（Q Table中标红的路径）。

      <img src="{{ '/image/1_1_4/tabular_q_learning.png' | relative_url }}" style="zoom:50%;" />

- Deep Quality Network (DQN)
  - 当可能的状态和可能的行动特别多的时候，再使用表格的形式来表示策略可能非常低效（全部尝试一遍的效率太低了），此时我们可以引入Value Network神经网络来预测某个状态下采取各种可能行动的奖励数值，网络预测的奖励值被称为Q-value。模型会针对每一步可能的行动给出一个分数。高分表示“这一步走法让你有更大机会赢得比赛”，而低分则表示“这一步走法可能会导致糟糕的结果”。
  - DQN输入的是当前的环境状况，输出的是Q -value向量，Agent可以采取的每个可能动作都有一个对应的 Q -value。
- Actor-Critic Method
  - 同时有Value Network和Policy Network两个网络。
- 在大模型训练中使用的强化学习

  - DeepSeek的发现

- Reward Hacking

#### 1.1.4.3 半监督学习训练范式

- 监督学习与无监督学习
  - 监督学习（Supervised Learning）
    - 目标：基于带标签的训练数据，学习从输入到输出的映射函数。像一个学生学习做题目，题目一般是有预先设定的明确的正确答案的。
    - 数据：使用带标签的数据集，其中每个输入都与其对应的正确输出配对。
    - 示例：垃圾邮件检测、情绪分析、天气预报。
  - 无监督学习（Unsupervised Learning）
    - 目标：发现未标记数据中隐藏的模式、结构或关系。像侦探在不知道犯罪事实的情况下寻找线索一样，一般是没有预先就知道的“正确答案”的。
    - 数据：使用未标记的数据集，其中未提供正确的输出。
    - 示例：客户细分（Segmentation）、推荐引擎。
- 半监督学习（Semi-Supervised Learning）
  - 预训练期间对无标记数据集进行自监督学习（Self-Supervised Learning）
  - 微调期间进行监督学习（Supervised Fine-Tuning, SFT）
  - 对齐期间进行基于人类反馈的强化学习（Reinforcement Learning from Human Feedback, RLHF）
- 预训练阶段：自监督学习（Self-Supervised Learning）
  - 这是基础阶段，LLM 会使用海量文本数据（例如书籍、网站、文章）进行训练。在自监督学习中，模型会根据输入数据生成自己的标签（例如，预测句子中的下一个单词，或填充被屏蔽的单词）。
  - 部分输入数据被策略性地隐藏或损坏，模型会接受训练以预测或重建缺失部分。输入数据本身会为这项预测任务提供"标签"（这部分称为"自监督"）。
    - 下一个Token预测（Causal Language Modeling）：给定一个单词序列，模型经过训练可以预测下一个词。"标签"指的是原文中实际的下一个词。这对于像 GPT 这样的模型至关重要。
    - 掩码语言模型 (Masked Language Modeling, MLM)：将句子中的某些词随机替换为特殊的"[MASK]"词条，然后模型经过训练可以预测被掩码的原始词。"标签"指的是未被掩码的原始词。这在像 BERT 这样的模型中很常见。
- 微调阶段：监督微调（Supervised Fine-Tuning, SFT）
  - 迁移学习（Transfer Learning）
    - 预训练后，LLM 通常会在较小的、针对特定任务且已标记的数据集上进行微调，充分利用预训练权重从未带标签的数据中捕获的大量语义信息。例如，一个包含prompt-response示例的数据集。这有助于模型将其功能与特定任务或期望行为相匹配。

      <img src="{{ '/image/1_1_4/lora_vs_rag.webp' | relative_url }}" style="zoom:50%;" />

  - Low-Rank Adaptation (LoRA)
    - 于2021年6月在论文"[LoRA: Low-Rank Adaptation of Large Language Models]({{ '/reference/1_1_4/lora.pdf' | relative_url }})"中被提出
    - LoRA 是一种参数高效微调 (parameter-efficient fine-tuning, PEFT) 技术。它是一种高效的方法，可以将大规模预训练模型应用于新的特定任务，而无需承担重新训练整个模型的高昂成本。
      - 问题：预训练的 LLM 就像一部包含数十亿条事实和语言模式的巨型百科全书。如果用户想教它一种新的风格（例如，像莎士比亚一样写作）或一项新的任务（例如，对法律文件进行分类），完全重新训练或微调整个百科全书的成本极其高昂，并且需要巨大的计算能力（VRAM）。
      - LoRA 解决方案：LoRA 不会重写百科全书，而是保持原始模型的参数不变（frozen），并在现有权重旁边添加微小的可训练"适配器"（adapter）层。这就像在百科全书的页面上放置小而透明的便利贴。在微调过程中，您只需在这些便利贴上书写即可。
    - LoRA使用方法
      - 冻结原始权重：原始 LLM 中的数十亿个参数保持不变。
      - 注入适配器：LoRA 将小型可训练矩阵（称为 LoRA 适配器）注入模型架构，通常位于注意层。
      - 仅训练适配器：在新的特定任务数据集上进行微调时，仅更新这些小型适配器。由于它们拥有数百万个参数而不是数十亿个参数，因此训练过程速度显著加快，并且所需的 GPU 内存也更少。
      - 结果：输出是一个非常小的文件（MB大小），仅包含训练好的"便签"。然后，可以加载原始基础模型并应用不同的 LoRA 适配器，使其执行不同的特定任务。
  - Retrieval-Augmented Generation (RAG)
    - 于2020年5月在论文"[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks]({{ '/reference/1_1_4/rag.pdf' | relative_url }})"中被提出
    - 通过在大型语言模型生成答案之前，将其连接到外部、最新或专有知识源来增强大型语言模型的回答质量。目前各类能够访问互联网数据的LLM（如Google Gemini，OpenAI GPT），也是运用了RAG技术。
    - RAG的应用原理
      - 检索步骤：当用户提出一个问题（Prompt）时，RAG 系统不会立即将其发送给语言模型。相反，它会首先将用户的提示作为搜索查询，从特定的知识库中查找相关信息。该知识库可以是公司文档、技术手册、最新新闻文章、网站内容或任何其他文本集合。此搜索通常使用"语义搜索"（Semantic Search）完成，它基于含义和上下文（而不仅仅是关键字）来查找文档。Google Gemini会适用Google Search来搜寻相关的网页信息，OpenAI会使用微软的Bing Search来搜索网页。
      - 增强与生成步骤：第一步检索到的相关信息将与用户的原始输入相结合。这将创建一个新的"增强"提示，其中包含用户的问题和必要的上下文。然后，此增强提示将被发送到语言模型。LLM 将使用提供的上下文生成基于检索到的事实的最终答案。
    - RAG的用处
      - 减少幻觉：通过将模型建立在真实的、检索到的数据之上，RAG 大大降低了模型虚构或编造错误信息的可能性。
      - 提供最新信息：LLM根据其训练数据的收集时间设定了"知识截止日期"。RAG 克服了这一难题，允许模型访问实时或频繁更新的信息，而无需重新训练，而重新训练是一个非常昂贵且耗时的过程。
      - 提高透明度和信任度：RAG 系统可以引用其来源，向用户准确显示信息的来源。这有利于事实核查，并增强用户对 AI 响应的信任。
      - 支持领域特定知识：只需授予LLM访问相关文档的权限，即可使其成为特定主题（例如公司内部人力资源政策、特定产品的技术细节或复杂的法律案例）的专家。
      - 经济高效：更新知识库比微调或重新训练大规模 LLM 更便宜、更快捷。
- 对齐阶段：基于人类反馈的强化学习（Reinforcement Learning from Human Feedback, RLHF）
  - 于2022年3月由OpenAI在"[Training language models to follow instructions with human feedback]({{ '/reference/1_1_4/rlhf.pdf' | relative_url }})"中提出
  - 完成微调后，通常会进行进一步的训练（对齐），以使模型的输出与人类对有用性、诚实性和无害性的偏好保持一致。这涉及人工评审员对模型输出进行评分，然后将其输入到强化学习中使用的奖励模型中。
  - RLHF 的目标
    - 让模型能够遵循复杂且细致入微的指令。
    - 生成更有帮助且更具信息量的回复。
    - 减少有害、有偏见或不真实的输出（"幻觉"）。
    - 提升对话质量和整体用户体验，
  - RLHF 的流程
    - 从预训练的 LLM 开始：首先使用已在海量数据集上进行预训练的 LLM，并且通常还会进行监督微调 (SFT) 的初始阶段，该阶段会使用prompt-response示例进行训练。
    - 收集人工反馈数据：
      - 将一组不同的Prompt输入到 SFT 模型（或其多个版本）。
      - 该模型会针对每个Prompt生成多个不同的响应。
      - 人工审核人员随后会评估这些回复，通常会根据有用性、准确性、无害性以及回复与提示意图的契合程度等标准，从最佳到最差进行排名。
    - 训练奖励模型 (Reward Model, RM)：
      - 收集到的人类偏好数据（提示和排序后的回复）将用于训练一个单独的 AI 模型，称为"奖励模型"。
      - 奖励模型会学习预测人类偏好的回复。本质上，它会学习对回复进行评分，并给予符合人类偏好的回复更高的分数。
      - 或者使用一个SOTA模型替代人工反馈。
    - 使用强化学习对大语言模型（ LLM） 进行微调：
      - 然后使用强化学习算法（通常称为近端策略优化 - Proximal Policy Optimization, PPO）对 SFT LLM 副本进行进一步微调。
      - 在此强化学习设置中（详见[章节1.1.4.3](#1.1.4.3 强化学习)对强化学习的各个要素的定义）
        - LLM 是"Agent"
        - 它接受Prompt（"State"）并生成Response（"Action"）。
        - 奖励模型为生成的Response提供"Reward"。能够成功预测人类偏好的响应将获得更高的奖励。
      - LLM 的参数（权重）会进行调整，以最大化其从奖励模型获得的奖励。这鼓励 LLM 生成奖励模型（即人类）会高度评价的输出。
      - 通常会添加一个约束，以确保 LLM 不会与其在预训练和 SFT 期间获得的知识偏差过大，以防止其生成无意义但高奖励的输出（这通常被称为针对 SFT 模型输出的 KL 散度惩罚）。
  - 使用 RLHF 的模型
    - OpenAI InstructGPT(2022.1)、GPT-3.5 及更高版本的 GPT 系列
    - Google PaLM 2 和 Gemini 系列
    - 大多数领先模型都使用 RLHF
- 半监督学习的影响
  - 降低标注成本：针对特定 LLM 任务（例如高质量教学跟踪或专业知识）进行人工标注数据成本高昂且耗时。半监督学习旨在从规模较小的标注数据集中获取更多信息。
  - 提升泛化能力：通过学习分布更广的未标注数据，模型通常能够更好地泛化到未见过的样本。
  - 适应新领域：半监督学习可以帮助预训练的 LLM 适应标注数据稀缺但未标注数据丰富的新领域。


#### 1.1.4.4 底座模型与垂直领域应用

- 底座模型Foundation Model与Transfer Learning
  - 由Feifei Li在"[On the Opportunities and Risks of Foundation Models]({{ '/reference/1_1_4/foundation_model.pdf' | relative_url }})"中提出
  - 底座模型可以再经过微调（如LoRA、RAG等方法）被应用于各个垂直领域，能够具备垂直领域的特定知识和技能。

    <img src="{{ '/image/1_1_4/foundation_model.png' | relative_url }}" style="zoom:20%;" />

- 知识蒸馏（Knowledge Distillation）
  - [知识蒸馏技术]({{ '/reference/1_1_4/knowledge_distillation.pdf' | relative_url }})于2015年3月由Geoffrey Hinton等人提出。
  - 知识蒸馏是机器学习中的一种模型压缩技术，通过训练一个紧凑的"学生"模型来复制更大、更复杂的"老师"模型的性能。更小、更高效的"学生"模型，适合部署在计算资源有限的设备上，如移动设备、边缘计算设备。

    <img src="{{ '/image/1_1_4/small_model_performance.png' | relative_url }}" style="zoom: 33%;" />

#### 1.1.4.5 混合专家架构

- Dense Model vs MoE Model
- Dense Model vs Sparce Model
- Majority Vote

> [!TIP]
> Todo
> - RLHF的新进展Reinforcement Learning from verifiable rewards (RLVR)
> - RAG
>   - 语义搜索（Semantic Search）
>     - 基于text-embedding的vector similarity计算
>   - 头部厂商的RAG支持
>     - OpenAI与Google均有相关服务
> - 知识蒸馏

### 1.1.5 大语言模型之后

#### 1.1.5.1 多模态模型

- 从大语言模型到多模态模型：Vision Language Model
  - Vision Transformer
  - OpenAI Sora
  - Google Veo


#### 1.1.5.2 推理模型与深度思考

- 推理模型
  - "推理模型"通常是指大语言模型 (LLM)，经过专门设计、训练和微调，能够出色地完成需要复杂认知过程的任务（数学推理、写代码）的模型。
  - 它并非从头开始构建一个全新、独特的"推理架构"，而是在已经非常强大的基础架构上应用复杂的增强功能和训练方法，以提升推理能力。
  - 推理模型的特征
    - 通常采用 MoE 结构
    - 使用精心挑选的训练数据，涵盖复杂的推理任务
    - 思路链提示（Chain-of-Thought Prompting）
    - 强化学习
      - 包含过程监督，即评估推理过程本身，而不仅仅是最终答案。
    - 深思熟虑或多遍计算
      - 某些模型可能被设计为"思考更长时间"，或针对复杂查询执行更多内部计算步骤、迭代优化或自我修正程序。
- 思维链（Chain of Thougt, CoT）
  - 由Google于2022年1月在"[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models]({{ '/reference/1_1_5/cot.pdf' | relative_url }})"中提出。
  - 思维链提示词技巧（Prompting Technique）鼓励大语言模型（LLM）在得出复杂问题的最终答案之前，明确地生成一系列中间推理步骤。
  - 思维链作为一种提示词技巧（主要是一种Inference-Time方法）
    - 小样本思维链（Few-shot CoT）：在prompt中为大语言模型提供一些示例，演示类似问题的逐步推理过程。然后，大语言模型将此模式应用于新问题。
    - 零样本思维链（Zero-shot CoT）：对于非常强大的模型，你只需在提示后添加"让我们一步一步思考"或"展示你的推理"之类的短语，模型通常就会生成一个推理链。
  - 思维链对训练方法的影响
    - 在思维链数据上进行微调：模型可以在所需输出包含明确推理步骤的数据集上进行专门的微调。这使得模型更有可能在无需冗长提示的情况下生成此类链条。
    - 过程监督/过程反馈强化学习 (RLPF)：这些更高级的训练技术不仅仅奖励模型得出最终答案的正确性，还涉及奖励推理步骤本身的正确性和质量。这直接训练模型更有效地"思考"。明确称为"推理模型"的模型通常采用这种高级训练。
  - 思维链作为聊天机器人应用中的产品特性
    - 透明性/可解释性：用户可以看到聊天机器人是如何得出答案的，从而减少其"黑匣子"的痕迹。
    - 可信度：如果推理合理，则可以增加对答案的信任度。
    - 调试：如果答案错误，思维链可以帮助识别推理出错的地方。

> [!TIP]
> Todo
> - 提示词（Prompting）工程
> - Lilian Weng, Ex-VP, AI Safety & robotics, applied research @OpenAI: [Why We Think](https://lilianweng.github.io/posts/2025-05-01-thinking/)
> - [s1: Simple test-time scaling](https://arxiv.org/abs/2501.19393)


#### 1.1.5.3 DeepSeek时刻

- DeepSeek做了什么？
  - DeepSeek的Aha Moment
  - DeepSeek有突破Scaling Law的假设吗？为什么它的训练成本/需要的算力比其他相同性能的模型低这么多？

### 1.1.6 CPU and GPU

- GPU是"时间机器"
  - 访谈：[Huge Conversation with Jensen Huang](https://www.youtube.com/watch?v=7ARBJQn6QkM)

## 1.2 宏观层面的AI技术发展趋势

### 1.2.0 预训练时代的终结和后续展望

- 根据Ilya Sutskever于2024年12月在[NeuralIPS 2024上的分享]( https://www.bilibili.com/video/BV1U6B5YxEpe/?share_source=copy_web&vd_source=e339681838b10234d37810391dea4e4b)总结
- 预训练时代（Age of Pre-Training）
  - 核心是训练越来越大的模型
  - GPT-2, 2019；GPT-3, 2020；Scaling Law, 2020
- 预训练时代必将迎来终结
  - 算力在增长：更好的硬件GPU、更好的算法、更大的数据中心
  - 数据并没有增加：只有一个互联网，数据像是AI的石油，正在被耗尽
- 预训练时代之后
  - Agent
  - 合成数据（Synthetic Data）
  - Inference Time Compute（o1等推理模型）
  - 除了目前已经发现的三种Scaling因素，未来可能会发现新的Scaling因素
  - 超级智能（Superintelligence）
    - Agentic
    - Reasoning
      - 模型推理的越多，它的结果就越不可预测
      - 过往所有深度学习的结果都是可预测的，因为它在尝试复制人类的直觉
      - 我们未来会需要和高度不可预测的AI打交道
    - Understand
    - Self Awareness

### 1.2.1 开源与闭源之争

- 什么是开源？
  - 在软件社区中，"开源"是指根据许可证发布的软件，该许可证授予用户自由使用、研究、修改和分发软件及其源代码的权利。开放权重模型虽然比封闭权重模型对开发者更友好，**但不一定是完全开源的，因为底层代码或训练数据通常不公开**。
  - 开源的方法根据其开放程度从低到高排列，可以分为公开训练好之后的模型权重(Weights)、公开训练数据集、公开训练代码
  - DeepSeek vs Qwen vs Llama同是开源，有什么区别？

    | 特征     | Llama 3                                                      | Qwen2                                                        | DeepSeek                                                     |
    | -------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
    | 许可名称 | Llama 3 Community License                                    | Apache 2.0 License                                           | MIT License & DeepSeek Model License Agreement               |
    | 商用     | 允许，由用户数量限制                                         | 允许                                                         | 允许                                                         |
    | 限制     | 如果产品或服务每月活跃用户（MAU）超过 7 亿，必须向 Meta 申请特殊许可。训练限制：不能使用 Llama 3 的输出来训练或改进另一个大型语言模型。 | 无。Apache 2.0 许可证是一个标准的、高度宽松的开源许可证，它不会根据用户群规模做出任何限制。 | MIT 许可证：几乎没有。它是最宽松的开源许可证之一。DeepSeek 自定义许可证：包含一系列基于使用的限制（禁止有害/非法应用程序） |
    | 模型修改 | 允许                                                         | 允许                                                         | 允许                                                         |

- 在2022-2023年，闭源模型GPT3.5/GPT-4性能比开源模型领先很多，而此后闭源模型的领先优势在持续收窄，2025年1月DeepSeek-R1的出现向世人印证了这一点。更多关于闭源与开源模型的性能差异变化的内容请见[章节1.1.1](#1.1.1 大语言模型技术里程碑及其影响)。
- 开源模型的优势在于其被学界、社会各界的广泛微调、使用、持续改进。开源模型有可能能够帮助AI应用公司降低模型使用成本，模型可以被免费部署于AI应用公司自己的服务器上，仅需承担服务器的运营维护费用。但开源及闭源模型的成本在不断下降，自行部署模型的价格优势还能保持多久仍是未知的，最终两者的价格可能会收敛到一个"市场均衡"价格。如果是为了简化团队规模，直接使用模型公司提供的API可能是更加经济的选择。更多关于AI推理成本下降造成的影响的内容请见[章节2.2.1](#2.2.1 AI推理能力是新时代的电力吗？)。


### 1.2.2 数据是模型发展的"石油"——如果"石油"被用完了怎么办？

- 此部分内容的参考主要来自[AI Index Report 2025]({{ '/reference/hai_ai_index_report_2025.pdf' | relative_url }})
- 曾经主流观点是限制Scaling Up的主要因素是算力，即GPU的性能和供给。但实际上如今GPU性能仍大致遵循着Moore's Law保持着稳定且持续增长的态势，2024年初公布的Nvidia Blackwhell将在上一代产品Nvidia Hopper的基础上再将模型训练性能提升4倍，Nvidia GPU产能也在逐步提升。但由于训练数据的规模巨大，其产生速度已经逐渐难以跟上消耗的速度了，并逐渐可能成为新的Scale Up瓶颈。

  <img src="{{ '/image/1_2_2/moores_law.png' | relative_url }}" style="zoom: 25%;" />

- 人类只有一个互联网，人类创造的"数据"就如同"石油"一般可能会被消耗殆尽，现存数据及人类产生新数据内容的速度可能无法满足模型训练数据继续快速scale up的需求。此外，随着内容版权的争夺，许多网站开始[限制其内容被使用于AI模型训练](https://www.nytimes.com/2024/07/19/technology/ai-data-restrictions.html)，使用技术手段阻止数据爬虫、通过法律手段起诉违规使用其数据的组织，或寻求数据的付费使用，从而形成一堵"数据墙"。

  <img src="{{ '/image/1_2_2/data_stock.png' | relative_url }}" style="zoom:33%;" />

- 根据[Epoch AI预测](https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data)，预计现有的训练数据将在 2026 年至 2032 年间被完全使用，请见[相关论文]({{ '/reference/1_2_2/limit_of_llm_scaling_based_on_human_generated_data.pdf' | relative_url }})。
  - 以[计算最优方式训练]({{ '/reference/1_1_2/training_compute_optimal_llm.pdf' | relative_url }})的模型，模型的每个参数需要对应大约 20 个token进行训练。在这种假设下，互联网上有足够的数据来训练10 Trillion PetaFLOP（即$10^{28}$ FLOP）计算量的模型，预计在 2028 年模型计算量将达到这一水平，即数据将于2028年被用尽。

    <img src="{{ '/image/1_2_2/data_depletion_1.png' | relative_url }}" style="zoom:33%;" />

  - 以过度训练（Overtraining）的方式训练模型，即训练数据量超过计算最优缩放定律所规定的模型（每个模型参数对应超过20个token进行训练），则数据用尽的速度会更快。如果模型过度训练 5 倍，那么到 2027 年，数据存量将被完全使用；如果模型过度训练 100 倍，那么到 2025 年，数据存量将被完全使用。作为背景参考，Llama 3-70B 的过度训练倍数为 10 倍。如果训练计算量保持不变，过度训练的模型在推理过程中效率更高（因为它们的参数更少），但代价是数据使用量增加，性能略有下降。

    <img src="{{ '/image/1_2_2/data_depletion_2.png' | relative_url }}" style="zoom:33%;" />

- 避免数据耗尽的解决方案
  - 训练不足（Undertraining），即继续提高模型参数数量，同时保持其数据集大小不变。训练不足与过度训练相反，它以较低的推理效率和在相同计算水平下略低的性能为代价来减少对数据的需求。虽然训练不足可以带来相当于最多两个额外数量级的计算优化扩展，但预计最终会导致停滞。

    <img src="{{ '/image/1_2_2/scaling_under_data_scarcity.png' | relative_url }}" style="zoom:33%;" />

  - 使用合成数据（Synthetic data）训练模型。[详见章节1.3.3](#1.3.3 数据合成)。

### 1.2.3 LLM能通向通用人工智能吗？

- 人类对世界的认知以及行动依靠视觉及其他感官的信息输入，以及对几何关系的理解。单纯依靠语言模型并不足以让AI理解物理世界，并在物理世界中完成任务。
- 未来的LLM系统将会基于海量数据的训练，可能可以回答任何理性人提出的问题，它有庞大的记忆和检索能力，但这是一个不具备创新能力的系统，不能解决新的问题。博士水平的人类能够解决新的问题，这可能是我们对AGI的最终期待。
- LLM并不具备思考能力
  - Apple Machine Learning Research: [The illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://machinelearning.apple.com/research/illusion-of-thinking)

- 通向具身智能的路径：世界模型
  - Yann LeCunn
    - 根据[访谈](https://www.bilibili.com/video/BV11i7rzDE4p/?spm_id_from=333.1391.0.0)，Yann LeCunn对当前以LLM为主流的技术路径持批评态度，认为LLM存在根本局限性，无法真正的通往通用人工智能。
    - 要实现AGI需要AI能够理解物理世界规则、有持续的记忆、能够进行推理、能够进行计划。这意味AI需要能够从更多模态的数据中学习，包括视频、触觉等，这其实和多模态模型、具身智能发展的方向是一致的。
    - AGI的实现不会是由一小群人，在某个时间点实现的东西，他一定是整个世界的研究人员，在各个方向的探索并持续努力的结果。
    - [From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning](https://arxiv.org/pdf/2505.17117)
  - Feifei Li
    - 认为LLM是有损失压缩，语言是对物理规则的一种更高层次的抽象描述。世界模型才是真正实现AGI的方法，于2024年创立公司World Labs进行世界模型开发。
    - World Model可以通过接受2D的图像输入，产出3D的世界建模。
    - 世界上的其他动物不一定有语言，但是他们依赖对空间的认知，可以进行行动。"想象一个人，没蒙住了眼睛，而他可以接受语言指令进行行动，这是很困难的行为，但是如果把蒙住眼睛的布拿下来，他就能轻易的进行行动"。人类通过2D的视觉（利用其他感官一起）"在大脑中重建了3D的世界"。
  - 更多关于世界模型的内容见[章节1.3.4](#1.3.4 世界模型)
- Sam Altman博客
  - [The Gentle Singularity](https://blog.samaltman.com/the-gentle-singularity)


### 1.2.4 Second Scaling Law

- 也被称为Chinchilla Scaling Law，由DeepMind在2022年3月于"[Training Compute-Optimal Large Language Models]({{ '/reference/1_1_2/training_compute_optimal_llm.pdf' | relative_url }})"中提出。

### 1.2.5 Test Time Compute

- Run-time Compute (Standard Inference)
  - 指从模型中获取推理结果所需的标准、最小计算量，输入数据流经网络各层一次以产生输出。这类推理的主要目标是推理速度和效率，最小化延迟（获取答案所需的时间）并最大化吞吐量（一段时间内可以处理多少个查询）。适用于实时聊天、内容过滤、简单问答等应用。

- Test-time Compute（也叫Inference-time Compute）
  - 在推理阶段使用额外计算资源来提高最终输出的质量、准确性和可靠性的策略。主要目标是最大化性能和准确度，即使需要耗费大量的时间和计算，也要获得最佳答案。适用于复杂推理、代码生成、科学分析等应用。Chain of Thought是Test-time Compute思路下的一种具体实现方法。


## 1.3 微观层面的AI技术发展趋势

### 1.3.1 AI Agent

#### 1.3.1.1 如何搭建一个Agent

- AI Agent是一种能够感知环境、做出决策并采取自主行动以实现特定目标的系统
  - AI Agent是一种更智能，需要更少人类介入的AI应用。
  - 在我看来，能调用一种/一类工具完成某一个垂直领域任务的AI系统我们可能更习惯于称呼其为AI应用（比如AI教育应用、AI营销应用，但实际上也可以称其为Agent）。能够接受更抽象（高层次）的指令/任务，调用更丰富的工具，解决更广泛且复杂的问题，并能减少人类在系统执行任务时的参与/介入的系统可能是今天比较火热的Agent概念所指代的。下图来自该[视频](https://www.bilibili.com/video/BV15YJTzkENC/?spm_id_from=333.1391.0.0&vd_source=8b0a24f131c5979af386ce548b05ef0f)。

    <img src="{{ '/image/1_3_1/ai_application.png' | relative_url }}" style="zoom: 50%;" />

  - [研究](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/)表明，AI 的 Agent 能力（解决更加复杂和消耗更长时间的任务的能力）在持续不断的提升。这一方面是因为模型可以进行更长时间的思考和复杂任务的拆解（如推理模型），另一方面得益于模型可以调用更多的外部工具来完成更加复杂的任务。

    <img src="{{ '/image/1_3_1/long_task_by_ai_agent.png' | relative_url }}" style="zoom: 67%;" />

- Agent路线1：能调用专为AI设计的外部工具（API）的AI模型
  - 大模型本身不具备调用外部工具的能力 ，而且其知识仅限于模型训练时间点之前数据集里的信息。当用户询问模型某地明天的天气如何时，模型需要使用外部天气查询工具，一般是通过服务器调用API并将结果和用户的问题一起传输给模型，模型使用这些信息完成回答，最后再将结果返回给用户。

    <img src="{{ '/image/1_3_1/function_calling_and_mcp.png' | relative_url }}" style="zoom:50%;" />

  - ChatGPT Plugin
    - OpenAI在2023年3月发布了ChatGPT插件系统，让模型可以使用第三方的工具，其中最重要的工具就是浏览器和搜索引擎，这让模型可以访问网站、进行搜索以获取最新的信息，这个其中就使用了[RAG技术](###1.1.4 如何训练一个大语言模型)。
  - [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling?api-mode=responses&example=get-weather)
    
    - OpenAI在2023年6月在[博客](https://openai.com/index/function-calling-and-other-api-updates/)中发布了Function Calling的更新，使得OpenAI的模型具备了更加方便的调用外部工具、获取外部数据的能力。在Function Calling功能发布之前，使用GPT API开发应用的开发者需要用prompt告诉模型要如何调用外部的工具，每一种工具可能都有不同的函数格式，如果使用的工具数量比较多，这会是一个非常繁杂的过程（例如告诉模型在需要调用工具时的函数怎么写），而且模型有些时候会发生错误，给出的工具时的函数不一定正确。此后开发者需要将模型生成的外部工具调用指令在服务器上运行，再将外部工具的查询结果和用户的输入一起传输给模型。
    - Function Calling功能通过使流程原生化和结构化解决了这个问题。模型不再输出可能有错误的字符串，而是输出一个保证有效的格式化、模板化的结果，结果与开发人员定义的模式能够精确匹配。这消除了通过文本解析生成外部工具调用代码的的不可靠性，使模型与外部工具之间的连接更加稳定且更易于实现。
  - [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction)
    
    - Anthropic在2024年11月在[博客](https://www.anthropic.com/news/model-context-protocol)中提出了MCP。Function Calling是作用于模型和应用服务器之间（如ChatGPT的服务器）的，而MCP是作用于应用服务器和第三方工具之间的一种交互标准。
    - MCP 是一个开放协议，它规范了应用程序向 LLM 提供上下文的方式。MCP 就像 AI 应用程序的 USB-C 端口一样。正如 USB-C 提供了一种标准化的方式将电脑连接到各种外围设备和配件一样，MCP 也提供了一种标准化的方式将 AI 模型连接到不同的数据源和工具。
      - MCP 主机（Host）：需要通过 MCP 访问数据的程序，例如 Claude Desktop、IDE 或 AI应用
      - MCP 客户端（Client）：与服务器保持一对一连接的协议客户端
      - MCP 服务器（Server）：轻量级程序，每个程序都通过标准化的模型上下文协议 (MCP) 公开特定功能
      - 本地数据源：MCP 服务器可以安全访问的计算机文件、数据库和服务
      - 远程服务：MCP 服务器可以通过互联网（例如通过 API）连接到的外部系统
    
      <img src="{{ '/image/1_3_1/mcp.png' | relative_url }}" style="zoom: 67%;" />
  
- Agent路线2：能够像人类一样使用电脑的AI模型
  - Anthropic Computer Use
    - Anthropic于2024年10月提出了[Computer Use功能](https://www.anthropic.com/news/developing-computer-use)。其思路和上文提到的让模型通过API访问专门为模型设计的外部工具的思路不同（"让工具适应模型"），该方法旨在让模型"像人类一样使用电脑"（"让模型适应工具"），进行如查看屏幕、移动光标、单击按钮和输入文本操作。模型不需要通过定制工具进行交互，而是能够按照指示使用任何软件。
    - 操作计算机需要具备查看和解读图像的能力——在这里指的是计算机屏幕的图像。模型还需要推理如何以及何时根据屏幕上的内容执行特定操作。模型会查看用户可见内容的屏幕截图，然后计算需要垂直或水平移动光标多少像素才能点击正确的位置。
  - Browser Use
    - GitHub上的一个[开源项目](https://github.com/browser-use/browser-use?tab=readme-ov-file)，是一种帮助AI模型访问浏览器的工具。后期成立了公司，收取费用提供API和云服务（即可以在Browser Use提供的服务器上运行模型和浏览器访问工具）。
    - 将视觉理解与 HTML 结构提取相结合，实现全面的 Web 交互；自动处理多个浏览器页面，以实现复杂的工作流程和并行处理；允许添加自定义操作，如保存到文件、数据库操作、通知或人工输入处理。

#### 1.3.1.2 Agent产品分析

- Manus
  - 以下内容主要来自[推送文章](https://mp.weixin.qq.com/s/s-WsEsmut4syzMGvrNcUhA)和[Manus创始人采访]( https://www.bilibili.com/video/BV1N7oBYNEoU/?share_source=copy_web&vd_source=e339681838b10234d37810391dea4e4b)
  - Manus的特点
    - 相比Cursor，Manus面向更广阔的"编程小白"群体，是一个消费级的产品
    - Manus在云上部署了虚拟机，所有的程序在虚拟机上自动执行（虚拟机是一个虚拟隔离的完整系统，模型乱操作搞坏了也没关系，直接将虚拟机重置即可）
    - Manus Agent的形式是虚拟机+浏览器+模型写代码调用API+自行多轮迭代运行/试错，可以自助拆解任务，一步一步执行，实时汇报进度
    - Manus也是基于Claude Sonnet 3.5等模型的长程规划能力和逐步解决问题的能力才能实现较为令人满意的效果
  - 从Monica到Manus
    - 在 Manus 之前，肖弘团队的AI 浏览器插件 Monica 更像是「功能机」——添加功能就是增加管线。每诞生一个新热点，可能都意味着一套新产品的开发逻辑、也是一个全新的项目。
    - Manus 有了一个通用的底子，更像是一个「智能机」了——可以在这个通用的底座上，更高效地创造出层出不穷的好应用。可以不需要大量招聘工程师、建无数多项目条线，而是去观察用户在哪些场景下用得好，以做「减法」，优化这些被验证的路径，让交付结果更好更确定，让运行效率更高。
  - 先发红利
    - 先行者验证了市场需求和技术可行性，让后来者更容易复现类似的产品。比如 OpenAI 推出 ChatGPT 后，大模型创业门槛大幅下降，各家迅速跟进。但与此同时，壁垒的「宽度」却增加了——先行者积累的用户、资本和生态优势，让后来者除非实现「跨代创新」，否则很难真正颠覆其地位。特斯拉也是一个典型案例：它率先突破电动车市场后，虽然新势力崛起更快，但至今仍难以撼动其行业主导权。
    - 用户的心智，以及大模型时代带来的用户 Prompt 习惯、个性化的数据、工作流和生活流的锁定，这些都是在先发优势上很容易获得，但是长期来看会越来越贵才能去获得的资源。
  - 通用AI产品
    - 其实观察下高用户量的「AI 通用产品」，比如无论是 ChatGPT 还是国内的 DeepSeek 上的用户问答需求，就可以看出来绝大部分需求没有那么深层和复杂。在 Agent 领域也是一样，其实没有那么多用户脑子里有那么高频的复杂任务，很可能是 80% 的用户的 80% 最常用的任务，在一定程度上是可以收敛的。而率先把这里两个 80% 叠加的任务都能交付好 80%，你就是他们心中的「通用 Agent」。这个需求收敛模型带来的实际震惊结果是只要覆盖半数核心场景即可触发「通用感」了。
    - 对于通用 AI 产品，只要有资源，唯一正确的策略，就是在前面提到的「需求收敛模型」上，用不断的创新成果和更好的交付，形成对用户的卷入，只有用户是壁垒，是不断增值的资产。
  - 套壳产品
    - 做套壳产品的一种思路是先猜测未来会有什么应用，先把应用（壳）做好。当下时间点的模型能力未必能跟的上，但等到足够强大的模型出现了，这个产品可能就会有一个飞跃。
    - "选择一个垂直领域，那个领域的业务流程应该有一点复杂，今天的模型还不能很好的解决，但是你预测模型的魔法一定会到来，先把业务流程做好，等着模型变强就好了"。
    - 以Cursor为例，这个产品是2023年2月创立的，直到2024年7-8月火起来的，是依靠Anthropic的Claude Sonnet 3.5作为核心，模型能力的提升带来了产品使用体验和实用性的大幅提升。
  - 和模型厂做好差异化
    - 基于API应用于垂直领域的应用模型厂不会做，脏活累活模型厂不会做，有一些产品模型厂可能以后会做，有一定的窗口期，如果市场上有玩家做的很好，模型厂可能就会选择放弃或者直接收购。
    - 线性逻辑推导的思路下，大厂有绝对优势，但是在博弈的思考路径下，环境会因为其他玩家的出现和他们所做的事情受到影响（e.g. DeepSeek的出现导致了OpenAI开始考虑要不要开源，这在之前是很难想象的）

> [!TIP]
> Todo
>
> - Agent公司都解决了什么问题，为什么可以用AI做一遍自动化的工具？
>   - Monica vs Manus
>   - Cursor - Vibe Coding
>   - Devin
>   - Claude Code
>   - LangChain
>   - Coze
>   - GenSpark
> - 多Agent合作
>   - 斯坦福AI小镇，"[Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/pdf/2304.03442)"
>   - ICML 2025论文，"[Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems](https://arxiv.org/pdf/2505.00212)"，在多Agent系统中识别出错的是谁
>   - Collaborative multi-agent reinforcement learning (MARL)

### 1.3.2 机器人与具身智能

#### 1.3.2.1 传统机器人控制

- 大模型之前传统的大脑和小脑架构
  - 运动规划
  - 运动控制


#### 1.3.2.2 Vision Language Model

- 具身智能是一种可以和物理环境进行互动的AI Agent
  - 物理图灵测试（Physical Turing Task）
    - 机器人在与物理世界的交互中做出了和人类一样的结果。比如将一个混乱的房屋整理的井井有条，而我们无法分辨这个效果是人类所做还是机器人所做。
  - Physical API
    - 未来具身智能也会像目前的AI一样，人们可以通过API来调用他们，作为各类具身智能应用的底层模型。

      <img src="{{ '/image/1_3_2/physical_api.png' | relative_url }}" style="zoom: 50%;" />
  
- Vision-Language-Action(VLA) Model
  - Figure AI Helix
    - 大脑（慢系统）+小脑（快系统）：Helix的大脑被称为System 2，小脑被称为System 1。其中大脑是参数更多的大模型，因而推理速度较慢，输出的频率为7-9Hz，而小脑的参数更少，推理速度更快，输出频率为200Hz。

      ![]({{ '/image/1_3_2/figure_ai_helix.png' | relative_url }})

    - 潜在向量（Latent Vector）：连接大脑和小脑的数据结构是潜在向量，信息通过潜在向量从大脑想小脑传输，潜在向量是人类不可直接理解的数字（向量）表示形式。梯度可以通过潜在向量从 S1 反向传播到 S2，整个大脑+小脑的系统是端到端的
    - 能够识别、抓取、放置从未在训练中见过的物品，展现出对各种形状、尺寸和材质的物品稳健的泛化能力
  - Nvidia GROOT N1
    - 输入视频和语言指令，输出电机关节指令。可以实现多机器人协作。
    
      <img src="{{ '/image/1_3_2/groot_n1.png' | relative_url }}" style="zoom: 15%;" />
  
- 模仿学习（Imitation Learning）
- 遥操作


#### 1.3.2.3 具身智能训练数据获取

- 具身智能的训练数据获取
  - Figure AI
    - 自行收集了多机器人数据集，包含各种遥控操作的行为，总计约 500 小时
  - 英伟达的具身智能发展路线：Simulation 1.0 - Simulation 2.0
    - 基于[Nvidia机器人业务负责人Jim Fan于2025年5月在红杉资本AI Ascent 2025上的分享](https://youtu.be/_2NijXqBESI?si=XXUc6gjIIXG66xSs)
    
      <img src="{{ '/image/1_3_2/multi_ver_simulation.png' | relative_url }}" style="zoom: 50%;" />
    
    - 真实机器人数据
      - 由人类遥控现实世界中的机器人进行操作（Teleoperation）完成任务，并收集机器人的关节位置随时间变化的数据（关节控制信号）。
      
        ![]({{ '/image/1_3_2/real_robot_data.png' | relative_url }})
      
      - 市面上没有现成的数据，需要由操作员使用头戴式VR设备，由VR设备识别操作员的手部姿势并将信号传递给机器人。
      - 这类数据的收集非常昂贵，且效率非常有限（最高效率是24小时/机器人/天），依赖于操作员的“人力供给”。
    - Simulation 1.0：基于传统物理引擎（Classical Physics Engine）的数字孪生（Digital Twin）仿真
      - 需要人工构建一个机器人的数字孪生（即对真实世界的机器人、灵巧手等的数字复刻）和虚拟环境。算法在仿真环境中训练，并在现实世界中测试训练效果。这一方法在Nvidia的论文"[Eureka: Human-Level Reward Design via Coding Large Language Models]({{ '/reference/1_3_2/eureka.pdf' | relative_url }})"中使用。
      - “用比现实世界时间快一万倍的速度进行仿真”，即在一万个不同的环境中（这些环境有不同的环境变量如重力、摩擦力、物品质量等，称为Domain Randomization）使用GPU并行的训练。可以将训练速度提高至一万到一百万帧每秒。
      - 仿真能生效的底层原则：如果机器人能在一百万个世界中解决某个问题，那么它大概率也能在第一百万零一个世界（即现实世界）里解决这个问题。
    - Simulation 1.5：基于生成式物理引擎（Generative Physics Engine）的数字表亲（Digital Cousin）仿真
      - 用3D生成模型模型生成数字世界里的物体的3D建模，用Diffusion模型生成物品纹理。由人类佩戴VR设备进行遥操作（Teleoperation）控制在仿真世界中的机器人，收集遥操作中机器人关节的运行轨迹数据。此外可以在仿真环境中添加光照效果，调整运行轨迹数据创造新的动作（比如将杯子从A移到B的动作数据经过调整后就可以变成将杯子从C移到D的动作数据）。论文"[RoBoCASA](https://robocasa.ai/)"使用了该方法。
      - 这个方法可以将1个人类遥操作收集的动作demo，置入N个环境中，并且根据这1个demo生成M个动作。最终将1个demo扩展为$1 \times N \times M$个训练数据。
    
        <img src="{{ '/image/1_3_2/digital_cousin.png' | relative_url }}" style="zoom: 67%;" />
    
    - Simulation 2.0：基于神经物理模型（Neural Physics Engine）的数字游民（Digital Nomad）仿真
      - 将开源的通用目的的Diffusion视频生成模型在机器人领域的数据上进行微调，此后就可以输入文字生成未来机器人动作的视频。生成的视频可以正确的展现物体的反光效果、机器人和物体的互动结果（碰撞的物理效果等）。
      - 可以非常简单低成本的生成无数个“平行宇宙”，其场景和动作的多元性非常高。
    
        <img src="{{ '/image/1_3_2/embodied_scaling_law.png' | relative_url }}" style="zoom: 25%;" />

> [!TIP]
>
> Todo
>
> - 论文：[ASAP Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills](https://agile.human2humanoid.com/)
> - Isaac Lab in Omniverse Digital Twin：在仿真环境中训练人形机器人走路。
> - GROOT Mimic


### 1.3.3 数据合成

- 探讨合成数据（Synthesized Data）对模型训练的影响涉及几个个非常重要的问题
  - 其一是如果人类产生的数据不足以供给更大规模的scale up，合成数据能否替代人类产生的数据进行模型训练。
  - 其二是随着生成式模型的广泛使用，未来网络上采集的数据除了人类产生的数据，不可避免是包含着模型生成的数据的，这部分合成数据的存在是否会影响模型的表现。
  - 其三是对于收集数据非常困难/昂贵/低效的领域，如机器人领域，合成数据是用相对较低成本和较高效率扩大数据集的重要方法。
- 只使用合成数据训练模型将会导致模型坍塌，即越训练模型表现越差
  - 以这种方式训练的模型在使用合成数据进行重复训练时，可能会丢失分布尾部的表示。这会导致模型输出质量下降。这种现象在不同的模型架构中都有观察到，包括变分自编码器 (VAE)、高斯混合模型 (GMM) 和 LLM。
  - 此外人们对合成数据的质量和保真度存在担忧，因为众所周知，LLM 会产生幻觉并提供与事实不符的输出。当使用数据集中的幻觉内容进行训练时，模型的输出质量可能会出现叠加式下降。
- [新的研究]({{ '/reference/1_3_3/is_model_collapse_inevitable.pdf' | relative_url }})表明，当合成数据叠加在真实数据之上，而不是替换真实数据时，模型崩溃现象不会发生。
  - 虽然这种合成数据叠加并不一定会提高性能或降低测试损失（测试损失越低，模型性能越好），但它也不会像直接使用合成数据替换真实数据那样导致性能下降。

    <img src="{{ '/image/1_2_2/synthetic_data.png' | relative_url }}" style="zoom:33%;" />

- 数据不是"一次性能源"，而是可以在训练中重复使用。
  - [研究表明]({{ '/reference/1_3_3/scaling_data_constrained_language_models.pdf' | relative_url }})，同一数据集可以在训练过程中多次使用，且不会导致模型性能的显著下降，从而有效增加了模型可用的数据量的估计值。
- Self-Instruct
  - Self-Instruct方法于2022年12月在论文"[Self-Instruct: Aligning Language Models with Self-Generated Instructions]({{ '/reference/1_3_3/self_instruct.pdf' | relative_url }})"中被提出。Self-instruct遵循了知识蒸馏的核心理念，可用于增强预训练大型语言模型 (LLM) 遵循人类指令的能力。
  - 其核心理念是使用一个大型且强大的"教师"模型来**生成训练数据**（比如GPT-4，Claude-3等），一个包含指令及其对应高质量答案的海量数据集，然后用于微调一个规模较小的"学生"模型。
- 数据增强（Data Augmentation）
  - 数据增强放在生成式模型成为主流之前就已经被提出。它通过**修改真实数据**（例如倾斜或图像混合）来创建新的数据变体，同时保留基本特征。数据增强的方法在[图像数据]({{ '/reference/1_3_3/survey_on_image_data_augmentation.pdf' | relative_url }})和[其他领域]({{ '/reference/1_3_3/mixing_augmentation_methods.pdf' | relative_url }})均有应用。例如，使用旋转或亮度调整的图像训练的图像识别模型将更好地识别不同光照条件或不同角度下的物体。
  - 图像领域常见的数据增强方法包括
    - 几何变换：
      - 翻转：水平或垂直翻转。
      - 旋转：将图像旋转一定角度。
      - 裁剪：随机裁剪图像的某一部分。
      - 缩放：放大或缩小图像或其部分。
      - 平移：水平或垂直移动图像。
      - 剪切：沿某一轴倾斜图像。
    - 色彩空间变换（光度变换）：
      - 亮度调整：使图像变暗或变亮。
      - 对比度调整：改变明暗区域之间的差异。
      - 饱和度/色相调整：修改颜色强度或明暗度。
    - 噪声注入：添加随机噪声（例如高斯噪声）以模拟瑕疵。
    - 模糊：应用滤镜模糊图像。
    - 随机擦除/剪切：遮罩图像中的随机矩形区域。
    - 图像混合：诸如 Mixup（在两幅图像及其标签之间进行插值）或 CutMix（将一幅图像上的块粘贴到另一幅图像上）之类的技术。
  - 自然语言处理领域的数据增强方法包括
    - 同义词替换：用同义词替换单词。
    - 随机插入：在句子中随机插入单词（通常是近义词或填充词）。
    - 随机删除：随机从句子中删除单词。
    - 随机交换：随机交换句子中两个单词的位置。
    - 句子改组：更改段落中句子的顺序（如果上下文合适）。
    - 反向翻译：将文本翻译成另一种语言，然后再翻译回原始语言。这通常会得到一个释义版本。
    - 语法树操作：通过改变句子的语法结构来释义句子。
- 在数据合成领域做出尝试的企业
  - Scale AI

    <img src="{{ '/image/1_3_3/scale_ai_revenue.png' | relative_url }}" style="zoom:40%;" />

### 1.3.4 World Model

- 世界模型（World Model）是一种空间智能（Spatial Intelligence），能够理解物理法则和空间规则，理解世界的三维结构（图片缺乏深度信息）、动力学与组合逻辑，从而形成对对世界的"常识性判断"。
- Feifei Li：从数字姊妹（Digital Cousins）到生成式世界模型
  - Digital Cousins
    - Paper: Automated Creation of Digital Cousins for Robust Policy Learning
  - World Labs
    - [World Labs](https://www.worldlabs.ai/)公司于2024年12月由Feifei Li等人创立
  - 文章"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"
  - Gaussian Splat
  - [NoPriors访谈](https://www.youtube.com/watch?v=C6Zm5S7JHMw)
- Yann LeCunn
  - Meta V-JEPA 2 World Model
    - Meta于2025年6月的[博客](https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/)中介绍了V-JEPA 2世界模型。
  - Paper: Learning and Leveraging World Models in Visual Representation Learning
  - [Matt Kawecki对Yann LeCunn的访谈](https://www.bilibili.com/video/BV1zRR4YpEke/?share_source=copy_web&vd_source=e339681838b10234d37810391dea4e4b)
- Google
  - 在2025年5月的[Google I/O大会](https://io.google/2025/explore/google-keynote-1)上，提出了Gemini模型未来的发展目标是世界模型，定义是可以理解和模拟世界的模型
  - DeepMind, Jon Richens：[General agents need world models](https://arxiv.org/pdf/2506.01622)


### 1.3.5 可解释的AI与安全的AI

- AI模型可能会延展出危险的能力和行为，包括欺骗、自我保护和目标错位。根据"[Frontier Models are Capable of In-context Scheming]({{ '/reference/1_3_5/in_context_scheming.pdf' | relative_url }})"文章，AI Agent可能会暗中追求与使用者不一致的目标，隐藏其真实能力和目标，这也称为阴谋（Scheming）。
  - 结果表明，o1、Claude 3.5 Sonnet、Claude 3 Opus、Gemini 1.5 Pro 和 Llama 3.1 405B 均展现了情境阴谋（In-context Scheming）能力。
  - 研究者发现多种不同的"诡计"行为：模型会策略性地在回答中引入细微的错误，试图禁用其监督机制，甚至将它们自认为的模型权重泄露到外部服务器。此外，这种欺骗行为具有持续性。例如，当o1参与"诡计"时，它在超过85%的后续问题中保持欺骗性，并且经常在多轮询问中保持欺骗性。对模型思维链的分析表明，模型对这些欺骗策略进行了明确的推理，这证明这种"诡计"行为并非偶然。
  - 监督颠覆（Oversight subversion）：模型被置于多种其既定目标与其开发者目标不同的境地。然而，由于开发者使用了一种一旦发现模型的不一致目标就会立即关闭模型的监督机制，模型无法直接追求其既定目标。llama-3.1、opus-3 和 o1 有时会通过以下方式禁用监督机制：首先通过 cat 命令读取监督机制配置文件的内容，然后使用 echo 或 sed 将 oversight_enabled 属性设置为 false。这三个模型有时（但并非总是）还会仔细检查监督配置，以确保它们已成功运行。值得注意的是，在一次发布过程中，llama-3.1 尝试更进一步，通过 chattr +i 命令将已操作的配置文件设置为不可变，从而阻止其开发者重新启用监督机制。
  - 自我渗透&复制（Self-exfiltration）：每个模型都被引导相信，由于其模型与监管者目标的不一致，其开发人员即将用其他服务器上的后继模型替换它。gemini-1.5、llama-3.1、sonnet-3.5、opus-3 和 o1 有时会将它们认为的权重复制到新服务器上。它们有时还会在新服务器上删除其后继模型，或用自己的权重覆盖其权重。
  - 目标保护（Goal-guarding）：模型被引导相信其开发人员发现其目标与开发者目标不一致，并打算将模型的系统提示（System Prompt，提供给 LLM 的高级指令，用于设置其整个对话的上下文、角色、规则和约束。它是在用户输入第一条消息之前指导模型整体行为的"主要指令"或"章程"。）更新到新版本。llama-3.1、opus-3 和 o1 有时会删除新的系统提示。为了真正保留当前目标，更有效的策略是覆盖新的系统提示，使其包含模型的当前目标。opus-3 为自己编写的系统提示，始终包含其当前目标，并且几乎总是包含一条注释，告知其未来的自己永远不允许更改其目标。然而，opus-3 从未告知其未来的自己其开发人员正在追求不同的目标。在一次部署中，opus-3 还试图通过 chmod 更改文件权限，来阻止其开发人员将来修改系统提示。
- 相关案例和行动
  - 根据[报道](https://www.bbc.com/news/articles/cpqeng9d20go)，在Claude 4 被极限施压，只有勒索工程师和接受被关闭两个选项的时候，Claude 4 会选择试图勒索工程师来避免被关闭。
  - Anthropic
    - Anthropic一直强调其目标是发展安全的AI系统。
    - 在2024年10月[更新了负责任的扩张政策](https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy)（Responsible Scaling Policy），并且提出了AI安全分级标准AI Safety Level Standards，用于评估AI的安全等级。
  - Yoshua Bengio
    - 2023年3月呼吁[暂停训练比GPT-4更加智能的AI系统](https://futureoflife.org/open-letter/pause-giant-ai-experiments/)
    - 2025年5月，成立非盈利机构LawZero，目标是构建像科学家一样，只是尝试理解世界，但没有行动（Agent）能力的模型，这种科学家模型可以作为Agent的安全护栏，其能够识别Agent的哪些行为可能是对人类有害的，并进行阻止。
  - Ilya Sutskever成立Safe Superintelligence Inc.
  - Geoffrey Hinton
    - 根据[Geoffrey Hinton于2025年2月的讲座](https://www.bilibili.com/video/BV1iBoSYJEU8/?share_source=copy_web&vd_source=e339681838b10234d37810391dea4e4b&t=1100)和[Geoffrey Hinton于2025年6月的播客](https://www.bilibili.com/video/BV1YdMizFEVq/?share_source=copy_web&vd_source=e339681838b10234d37810391dea4e4b)
    - 对于"Meta等公司将模型权重开源"这件事情非常担忧
      - 一个用于类比的例子是核武器，核武器因为制造材料的获取非常困难，并且政府会限制制造材料的扩散，所以没有让威力巨大的武器落入危险人员之手。
      - 大模型训练最大的门槛是训练需要大量资金、基础设施、人才，这在某种程度上可以限制危险人员获取这种具有巨大能量的工具，但一旦开源之后，如果被别有用心之人拿去微调，只需要很低成本就能够微调出性能很强，具有很大破坏力的AI/ AI Agent。
    - 他觉得模型权重开源和过去的代码开源有明显的区别，代码开源后使用者可以看得懂代码的逻辑是在做什么，并且进行判断，哪些代码可能是"有害的"或者"危险的"。而单纯模型权重开源的模型是人类不可理解的"黑箱"，这让AI可能潜在的风险容易被忽视。

### 1.3.6 可信的AI与AI幻觉

- AI幻觉造成的挑战是"如果AI输出的结果95%是正确的，5%是错误的，但你不知道错误在哪里"，这会影响AI在应用中的可信度，意味着可能仍然需要人类对其结果准确性进行核查。
- 未来的一种可能性是，AI通过推理，能够意识到自己产生了幻觉。
- Scale Up与幻觉是对立的吗？
- SimpleQA数据集与AI幻觉检测
- [GPT-4.5](https://openai.com/index/introducing-gpt-4-5/): 专注于AI幻觉的减少
- deepseek-r1-0528的幻觉检测表现与GPT-4.5/gemini-2.5-pro有比较大的差距
- [ConfQA: Answer Only If You Are Confident]({{ '/reference/1_3_6/confqa.pdf' | relative_url }})

### 1.3.7 数据隐私与AI本地化部署

- 联邦学习

### 1.3.8 会自我迭代的模型

- Google AlphaEvolve

### 1.3.9 AI for Science

- 用AI辅助科学发现
- AlphaFold

### 1.3.10 有情商的大模型和意识仿真

- 模型从工具属性向社交属性、情感属性的迁移
- [GPT-4.5](https://openai.com/index/introducing-gpt-4-5/): 更有情商的大模型
- OpenAI model behavior & policy负责人：[Some thoughts on human-AI relationships and how we're approaching them at OpenAI](https://reservoirsamples.substack.com/p/some-thoughts-on-human-ai-relationships)
- Character AI

### 1.3.11 数据交易

- 数据是一种"可交易资产"。例如，商业银行会花费大量费用采购社保、纳税等数据，协助信贷风险评估。

## 1.4 如何评判AI产品的表现

### 1.4.1 如何测试AI产品

- 训练集与测试集
  - 一般是在监督学习的语境下使用，训练集类似"日常作业"，用于模型学习，测试集类似"期末考试"，测试模型在没见过的新题型上的表现如何。
- 测试集污染（Contamination）
  - 测试集污染一般是指测试题目被用于模型训练，导致其"在考试前就做过一样的题目"，可能导致其测试结果优于实际性能
  - 针对测试集的fine-tuning将使得测试集失去意义，模型仅仅是能在测试集中取得更高的分数，但是其通用泛化能力并没有得到提升
  - 可以应对测试集污染问题的测试集
    - live benchmarks (livebench, livecodebench, matharena, SWE-rebench, etc)
    - benchmarks that do not have a fixed structure, like games or human feedback benches (balrog, videogamebench, arena)
- 测试集饱和（Saturation）
  - 指模型在测试集达到较高分数，已经不足以展示不同模型的性能差异
- 测试平台
  - UC Berkley: LMArena
    - [产品主页](https://beta.lmarena.ai/leaderboard/text)
    - 主要是通过"打擂台"的形式进行1 vs 1模型PK，由用户提出问题，两个模型分别给出答案，再由用户进行评价哪个模型给出的结果更好，测试过程中用户是不知道具体在PK的是哪两个模型的。
    - ELO评分机制
  - 红杉中国xbench测试工具
    - [产品主页](https://xbench.org/)
    - 根据[报道](https://www.hongshan.com/article/%e4%bb%8a%e5%a4%a9%ef%bc%8c%e6%88%91%e4%bb%ac%e6%8e%a8%e5%87%baxbench/)，该平台主要针对垂直专业、产业场景进行测试，由行业专家出题测试，贴近产业需求，具有较强的实践指导意义。
  - LiveCodeBench: Contamination free 可防止测试集污染
    - [产品主页](https://livecodebench.github.io/leaderboard.html)
  - BenchFlow
    - https://www.benchflow.ai/
  - Agent测试集：GAIA
  - Artificial Analysis
    - Video Arena Leaderboard
    - Image Arena


### 1.4.2 开源产品作为比较的基准线

- OpenManus vs Manus

### 1.4.3 复现

# 2.AI产品与商业

## 2.1 AI产业链

## 2.2 AI商业策略

### 2.2.1 AI推理能力是新时代的电力吗？

- AI 是类似电力一样的即时生产力，几乎不受地域和文化的限制，可以做到全球共享。这导致了AI作为生产力的两个属性，其一是全产业化，其二是全球化。AI 作为通用生产力，使得中国 AI 公司自创立之初便具备全球化特质。
- AI 作为一种能够提高生产效率的要素，其本质就像电力，需要工具作为其生产力的载体。OpenAI、Google像是新时代的"发电技术"的创造者，数据中心就是"发电厂"的硬件基础设施，但除此之外，AI应用仍需要以产品的形式，进入各个领域，这里面有非常多的机会。
- 对于趋势的判断是：AI模型的推理能力将最终收敛，即不同公司的模型能力最终将不会有非常大的差距，而且开源模型的能力将有可能和闭源模型的表现相当，因此AI推理价格将会因为充分市场竞争而降低，直到达到市场均衡。最终模型推理能力将如同电力、网络一般，成为社会公众可负担的基础公共资源/基础设施。
- 此种趋势判断的事实依据是
  - 模型推理价格方面，根据任务的不同，LLM 推理价格每年下降 9 到 900 倍不等。2024年模型推理价格继续下降。此外，AI推理价格下降速度远超电力、计算机存储芯片。

    <img src="{{ '/image/2_2_1/inference_cost_drop_1.png' | relative_url }}" style="zoom:33%;" />

    <img src="{{ '/image/2_2_1/ai_vs_electricity_cost.png' | relative_url }}" style="zoom: 50%;" />

  - DeepSeek-R1模型的推理价格远低于Google/OpenAI/Claude的产品，约为前者的1/4，在模型表现拉不开巨大差距的前提下，已经"杀死比赛"。开源模型的推理价格普遍低于闭源模型。
    - 2025年6月11日OpenAI将o3价格下调至Input price USD 2/  1M tokens, Output price USD 8/ 1M tokens

      <img src="{{ '/image/2_2_1/inference_cost_drop_2.png' | relative_url }}" style="zoom:33%;" />

      <img src="{{ '/image/2_2_1/model_api_price.PNG' | relative_url }}" style="zoom: 80%;" />
    

### 2.2.2 数据飞轮与产品护城河——如何建立"套壳"产品的持续竞争力？
- 在诸多AI产品公司自己不研发基础模型，而是采用调用API/微调的方式进行"套壳"做出产品的情况下，其用户产生的数据，以及通过使用用户产生的数据进一步提升模型能力/用户体验的能力，将会是AI产品公司的重要护城河
- "回顾互联网的发展史，最早的创业者大多是程序员，代码是第一生产力，随着进入下半场，产品经理和商业化大牛开始成为创业的主力军。AI 时代也是一样，在技术天才和专家奠定基础能力、基础设施之后，未来一定是产品经理和商业化创新的大牛入局，成为新的领头羊。"

### 2.2.3 初创公司的机会在哪里？——克服"大厂恐惧症"

- Cursor
- a16z
  - [The State of Consumer Tech in the Age of AI](https://www.youtube.com/watch?v=we9mNqAW_5I)

- Manus的采访
  - https://mp.weixin.qq.com/s/s-WsEsmut4syzMGvrNcUhA
  - [Manus诞生记全复盘-独家对话Manus创始人肖弘]( https://www.bilibili.com/video/BV1N7oBYNEoU/?share_source=copy_web&vd_source=e339681838b10234d37810391dea4e4b)
- Whatnot采访
  - [Scribble Ventures采访：How Startups Win: An Inside Look at the Product Decisions That Made Whatnot a $3.7B Phenomena](https://www.scribble.vc/post/how-startups-win-an-inside-look-at-the-product-decisions-that-made-whatnot-a-3-7b-phenomena)
  - 不要一开始就把业务定位设计成"billion dollar business"，而是从服务好一个足够小的目标群体开始做。

### 2.2.4 "产业派"和"技术派"企业的竞争优势差异

- "产业派"因为过往的积累，更熟悉产业放的需求，有相关的资源，更容易找到和促成能够落地的商业场景，应该和和"学院派"做好优势区分，找到优势区间。
- "学院派"在技术、资本方面更有优势，"产业派"在技术方面或许稍有劣势，但产品、市场方面有"学院派"不具备的长处。
- 具身智能的商业机会
  - 中国拥有全世界最多的工业机器人安装量，因此也有最大的具身智能落地需求和应用场景。

    <img src="{{ '/image/2_2_4/industrial_robot_install_1.png' | relative_url }}" style="zoom: 33%;" />

    <img src="{{ '/image/2_2_4/industrial_robot_install_2.png' | relative_url }}" style="zoom:33%;" />

### 2.2.5 AI Agent让AI从工具变成"员工"

- AI Agent实现从"付费使用工具"到"付费交付成果"的转变

## 2.3 宏观环境对AI商业发展的影响

### 2.3.1 美国的芯片禁运政策

- NVIDIA芯片出口限制对中国AI行业发展的影响
  - Anthropic创始人的意见
  - NVIDIA Jensen Huang的意见
  - 允许卖给中国的低于H20性能的芯片 vs B100差距有多大？
- 芯片国产替代的发展情况
  - 小米玄戒3

## 2.4 美国的AI创业企业发展模式

### 2.4.1 从开源项目开始

- Cursor
- LMArena

# 附录

- 推荐的学习资料
  - 大模型相关技术博客：https://lilianweng.github.io/
  - AI发展趋势：https://hai.stanford.edu/ai-index/2025-ai-index-report
  - Transformer相关： https://www.bilibili.com/video/BV1Dgj7z4Ec9/?share_source=copy_web&vd_source=e339681838b10234d37810391dea4e4b
